---
title: Explore Cognitive Structure
author: Liang Zhang
date: 2022-08-21
format:
  html:
    code-fold: true
    toc: true
    toc-location: left
categories: [structure, efa]
execute:
  warning: false
  showtext: true
bibliography: references.bib
---

```{r}
#| label: setup

library(tidyverse)
library(corrr)
library(showtext)
library(gt)
library(formattable)
library(patchwork)
library(psych)
requireNamespace("bit64")
```

```{r}
#| label: utils

pivot_wider_indices <- function(data, name_value = "test", always_suffix = FALSE) {
  data |>
    group_by(game_name) |>
    mutate(n_index = n_distinct(index_name)) |>
    ungroup() |>
    mutate(
      game_index = if (always_suffix) {
        str_c(game_name, index_name, sep = ".")
      } else {
        if_else(
          n_index == 1,
          game_name,
          str_c(game_name, index_name, sep = ".")
        )
      }
    ) |>
    pivot_wider(
      id_cols = user_id,
      names_from = game_index,
      values_from = .data[[name_value]]
    )
}

summary_data <- function(data, name_value = "test") {
  pivot_wider_indices(data, name_value, always_suffix = TRUE) |>
    select(-user_id) |>
    psych::describe() |>
    as_tibble(rownames = "game_index_name") |>
    select(game_index_name, n, mean, sd, min, max, skew, kurtosis) |>
    separate(game_index_name, c("game_name", "index_name"), sep = "\\.") |>
    mutate(
      skew_status = case_when(
        abs(skew) > 1 ~ "highly",
        abs(skew) > 0.5 ~ "moderately",
        TRUE ~ "acceptable"
      )
    )
}

show_normality <- function(data, game_name, index_name, skew, ...,
                           name_value = "test") {
  p_hist <- ggplot(data, aes_string(name_value)) +
    geom_histogram(bins = 30) +
    labs(x = "Raw Score", y = "Count") +
    theme_bw()
  p_qq <- ggplot(data, aes_string(sample = name_value)) +
    geom_qq() +
    geom_qq_line() +
    labs(x = "Expected Quantile", y = "Observed Quantile") +
    theme_bw()
  wrap_elements(grid::textGrob(
    str_glue("{game_name}\n{index_name}\nskewness = {round(skew, 2)}"),
    rot = 90
  )) + p_hist + p_qq +
    plot_layout(widths = c(0.1, 0.45, 0.45))
}

transform_indices <- function(x, game_name, index_name) {
  if (game_name == "小狗回家" & index_name == "mean_score") {
    return(-log10(1 - x))
  }
  if (game_name == "各得其所" & index_name == "mrt_init") {
    return(log10(x))
  }
  if (game_name == "数字推理") {
    return(sqrt(x))
  }
  if (game_name == "塔罗牌") {
    return(-sqrt(max(x, na.rm = TRUE) - x))
  }
  if (game_name == "人工语言-中级") {
    return(-sqrt(max(x, na.rm = TRUE) - x))
  }
  if (game_name == "语义判断") {
    return(-log10(max(x, na.rm = TRUE) + 1 - x))
  }
  if (game_name == "数感") {
    return(log10(x))
  }
  if (game_name == "时长分辨") {
    return(log10(x))
  }
  if (game_name == "节奏感知") {
   return(sqrt(x))
  }
  return(x)
}

calc_test_retest <- function(data,
                             name_test = "test",
                             name_retest = "retest") {
  data_no_missing <- data |>
    select(all_of(c(name_test, name_retest))) |>
    drop_na()
  outliers_result <- data_no_missing |>
    performance::check_outliers(method = "mahalanobis_robust")
  data_no_outlier <- data_no_missing |>
    slice(-which(outliers_result))
  icc_result <- ICC(data_no_outlier)
  data.frame(
    n_obs = icc_result$n.obs,
    icc = icc_result$results$ICC[[2]],
    r = cor(data_no_outlier[[name_test]], data_no_outlier[[name_retest]])
  )
}

vis_cor <- function(x, ...) {
  x |> 
    correlate(...) |>
    autoplot(
      method = "HC", 
      triangular = "full",
      low = "blue",
      high = "red"
    ) +
    theme(aspect.ratio = 1)
}

format_n_factors <- function(n_factor_result) {
  checks <- c("BIC", "SABIC", "eBIC", "MAP")
  n_factor_result$vss.stats |>
    as_tibble() |>
    select(BIC, SABIC, eBIC) |>
    mutate(n_factor = seq_len(n()), .before = 1L) |>
    add_column(MAP = n_factor_result$map) |>
    gt() |>
    fmt_number(
      -c(n_factor, MAP),
      decimals = 0
    ) |>
    fmt_scientific(MAP) |>
    tab_style(
      style = cell_text(weight = "bold"),
      locations = map(
        checks,
        ~ cells_body(
          columns = all_of(.x),
          rows = !!sym(.x) == min(!!sym(.x))
        )
      )
    ) |>
    as_raw_html()
}

plotly_efa <- function(model) {
  heatmaply::heatmaply_cor(
    t(unclass(model$loadings)),
    dendrogram = "column",
    k_col = NA,
    label_format_fun = function(...) round(..., digits = 2),
    margin = c(50, 50, 50, 0)
  )
}
```

```{r}
#| label: load-data

targets::tar_load(users)
targets::tar_load(indices_selection)
targets::tar_load(indices_struct)
dim_order <- c(
  "attention", "multitask", "switching", "inhibition",
  "reasoning", "complex span", "working memory", "short term memory",
  "long term memory", "probability learning",
  "speeded IP", "strategic IP",
  "perception", "math", "language"
)
indices_outlier_info <- indices_struct |>
  arrange(factor(dimension, dim_order)) |>
  group_by(game_name, index_name) |> 
  mutate(
    is_outlier = unclass(performance::check_outliers(test, method = "iqr"))
  ) |> 
  ungroup()
indices <- indices_outlier_info |> 
  filter(!is_outlier) |> 
  select(-is_outlier)
indices_stats <- summary_data(indices)
```

# Data Cleaning Summary

Careless responses will ruin data, especially when there are 10% or more participants respond carelessly [@woods2006]. From @fig-show-careless, it shows that there were two tasks in which more than 10% of participants performed badly (labelled `'unmotivated'` in the figure). These two games are known to be difficult. So we can conclude that participants were all motivated to fulfill all the tasks, there is no need to screen unmotivated participants.

```{r}
#| label: fig-show-careless
#| fig-cap: Counts of Participants with Careless Responses
#| fig-width: 8
#| fig-height: 6

targets::tar_load(user_resp_check)
user_resp_check |> 
  group_by(game_name, is_motivated) |> 
  summarise(n = n(), .groups = "drop_last") |> 
  mutate(prop = n / sum(n)) |> 
  ungroup() |> 
  arrange(desc(prop)) |> 
  mutate(
    game_name = as_factor(game_name),
    is_motivated = factor(is_motivated, c(FALSE, TRUE), c("Unmotivated", "Motivated"))
  ) |> 
  ggplot(aes(game_name, n, fill = is_motivated)) +
  geom_col() +
  geom_text(
    aes(label = scales::label_percent(accuracy = 0.1)(prop)),
    position = position_stack(vjust = 0.5)
  ) +
  scale_fill_brewer(palette = "Paired") +
  coord_flip() +
  ggpubr::theme_pubclean(flip = TRUE) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    legend.title = element_blank()
  )
```

Another issue is outliers. We screened out one-dimension outliers using Tukey's boxplot method, with a coefficient of 1.5, i.e., participants perform out of 1.5 inter-quantile range from the first and third quartiles are screened out. @fig-outlier-summary shows a summary of removed participants for all tasks, in which the task '节奏感知' stood out, with `r with(indices_outlier_info, sum(is_outlier[game_name == "节奏感知"]))` participants removed.

```{r}
#| label: fig-outlier-summary
#| fig-cap: Summary of Removed Outliers Count for Each Task
#| fig-width: 8
#| fig-height: 6

indices_outlier_info |> 
  group_by(game_name, index_name) |> 
  summarise(n_outlier = sum(is_outlier, na.rm = TRUE), .groups = "drop") |> 
  ggplot(aes(n_outlier)) +
  geom_histogram(fill = "white", color = "black", bins = 30) +
  theme_bw() +
  labs(x = "Count of Outliers", y = "Count of Task Indices")
```

# Data Checking on Normality

It is necessary to check normality before performing exploratory analysis.

## Original Form

```{r}
#| label: fig-histograms
#| column: page
#| fig-cap: Histogram of All Task Indices
#| fig-width: 10
#| fig-height: 20

indices |>
  ggplot(aes(test)) +
  geom_histogram(bins = 30) +
  facet_wrap(vars(game_name, index_name), scales = "free", ncol = 5) +
  labs(x = "Raw Score", y = "Count") +
  theme_bw()
```

```{r}
#| label: fig-qqplot
#| column: page
#| fig-cap: QQ-plot of All Task Indices
#| fig-width: 10
#| fig-height: 20

indices |>
  ggplot(aes(sample = test)) +
  geom_qq() +
  geom_qq_line() +
  facet_wrap(vars(game_name, index_name), scales = "free", ncol = 5) +
  labs(x = "Expected Quantile", y = "Observed Quantile") +
  theme_bw()
```

According to a classic book [@kline2016], critical value for absolute skewness is 3, and critical value for absolute kurtosis is 10; thus, these indices are all not too apart from normal distribution. But see @fig-histograms, we detect some heavily skewed indices. Just as a classic book [@bulmer1979] said, critical value for skewness is 0.5 (moderately skew) and 1 (highly skew).

```{r}
#| label: data-description
#| column: page

indices_stats |>
  mutate(across(mean:kurtosis, ~ digits(., 3))) |>
  formattable() |>
  as.datatable(
    caption = "Descriptive Statistics of All Indices"
  )
```

## Transformed

Here we decide to convert the moderately and highly indices based on the conversion suggested by Howell [-@howell2013].

```{r}
#| label: fig-normality-skewed
#| fig-cap: Normality Check on Suspected Skew Indices
#| fig-width: 8
#| fig-height: 18

indices |>
  inner_join(
    indices_stats |>
      select(game_name, index_name, skew, skew_status) |>
      filter(skew_status != "acceptable"),
    by = c("game_name", "index_name")
  ) |>
  group_nest(game_name, index_name, skew, skew_status) |>
  pmap(show_normality) |>
  wrap_plots(ncol = 1)
```

From @fig-normality-skewed, it might be a good choice to transform indices with an absolute skewness value larger than 0.5, and logarithmic and square root transformations are used.

```{r}
#| label: transform-data

game_needs_trans <- indices_stats |>
  filter(abs(skew) > 0.5)
indices_trans <- indices |>
  semi_join(game_needs_trans, by = c("game_name", "index_name")) |>
  group_by(game_name, index_name) |>
  group_modify(
    ~ .x |>
      mutate(
        across(
          c(test, retest),
          transform_indices,
          game_name = .y$game_name,
          index_name = .y$index_name
        )
      )
  ) |>
  ungroup() |>
  mutate(score = if_else(reversed == "yes", -test, test))
indices_stats_trans <- summary_data(indices_trans)
indices_pooled <- bind_rows(
  origin = indices,
  trans = indices_trans,
  .id = "index_type"
) |>
  left_join(
    game_needs_trans |>
      add_column(need_trans = TRUE) |>
      select(game_name, index_name, need_trans),
    by = c("game_name", "index_name")
  ) |>
  mutate(need_trans = coalesce(need_trans, FALSE))
```

```{r}
#| label: fig-normality-trans
#| fig-cap: Normality Check after Transformations
#| fig-width: 8
#| fig-height: 18

indices_trans |>
  inner_join(indices_stats_trans, by = c("game_name", "index_name")) |>
  group_nest(game_name, index_name, skew, skew_status) |>
  pmap(show_normality) |>
  wrap_plots(ncol = 1)
```

From @fig-normality-trans, we know these indices now perform very well on normality. The transformations seem to be useful.

# Data Checking on Reliability

```{r}
#| label: reliability-list
#| column: page

test_retest <- indices_pooled |>
  group_by(game_name, index_name, dimension, index_type, need_trans) |>
  group_modify(~ calc_test_retest(.)) |>
  ungroup()
format_na <- function(str, x) {
  ifelse(is.na(x), "-", str)
}
test_retest |>
  pivot_wider(
    id_cols = c(game_name, index_name),
    names_from = index_type,
    values_from = c(n_obs, icc, r),
    names_vary = "slowest"
  ) |>
  mutate(
    across(
      starts_with(c("icc", "r")),
      ~ digits(., 2, postproc = format_na)
    ),
    n_obs_trans = digits(n_obs_trans, 0, postproc = format_na)
  ) |>
  formattable(
    list(
      formattable::area(col = c(icc_origin, icc_trans)) ~
        color_tile("white", "pink", na.rm = TRUE),
      game_name = formatter(
        "span",
        style = ~ if_else(
          coalesce(icc_trans, icc_origin) < 0.5,
          "background:grey",
          NA_character_
        )
      )
    )
  ) |>
  as.datatable()
```

From the above table, it shows that the transformations are successful in elevating test-retest reliability. Kline stated in their book [-@applied2013]: "If $r_{xx} < .50$, then most of the total variance is due to measurement error. Indicators with such low score reliabilities should be excluded from the analysis." We should remove those indices with test-retest reliability less than 0.5 (grey background cells) according to this quote. The removed indicators are:

```{r}
#| label: indicators-removed

indicators_remove <- test_retest |>
  filter(!need_trans | (index_type == "trans" & need_trans)) |>
  filter(icc < 0.5) |>
  select(game_name, index_name, dimension, icc)
indicators_remove |>
  knitr::kable(
    digits = 2,
    caption = "Removed Indicators",
    row.names = TRUE
  )
```

```{r}
#| label: fig-scatter-low-reliability
#| fig-cap: Scatterplot of Low Reliability Tasks
#| fig-width: 6
#| fig-height: 15

indices_pooled |> 
  filter(!need_trans | (index_type == "trans" & need_trans)) |>
  semi_join(indicators_remove, by = c("game_name", "index_name", "dimension")) |> 
  drop_na() |> 
  ggplot(aes(test, retest)) +
  geom_point() +
  ggpmisc::stat_poly_line() +
  ggpmisc::stat_correlation(small.r = TRUE) +
  geom_abline(slope = 1, intercept = 0, linetype = "dotted", color = "red") +
  facet_wrap(vars(game_name, index_name), scales = "free", ncol = 3) +
  labs(x = "Test", y = "Retest") +
  theme_bw() +
  theme(aspect.ratio = 1)
```

Note there is a very special case, i.e. "我是大厨", whose pearson correlation coefficient is substantially larger than ICC value. Further analysis (see @fig-cook-test-retest) shows that participants improve more if they perform worse in the first test. In this case, it makes us doubt whether intraclass correlation (ICC) is suitable for cognitive ability tests for there is always practice effect (they do no really use parallel versions).

```{r}
#| label: fig-cook-test-retest
#| fig-cap: Special Case for Test-retest Reliability
#| fig-width: 6
#| fig-height: 6

indices_pooled |>
  filter(game_name == "我是大厨") |>
  ggplot(aes(test, retest)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x) +
  geom_abline(
    slope = 1,
    intercept = 0,
    linetype = "dotted",
    color = "lightblue"
  ) +
  theme_bw() +
  coord_fixed() +
  labs(x = "Test Phase", y = "Retest Phase")
```

But see @fig-scatter-icc-pearson, these two reliability indices are closely correlated with each other.

```{r}
#| label: fig-scatter-icc-pearson
#| fig-cap: Correlation Between ICC and Pearson's Coefficient
#| fig-height: 6
#| fig-width: 6

test_retest |>
  ggplot(aes(icc, r)) +
  geom_point() +
  stat_smooth(method = "lm", formula = y ~ x) +
  ggpmisc::stat_correlation(small.r = TRUE) +
  geom_abline(
    slope = 1,
    intercept = 0,
    linetype = "dotted",
    color = "lightblue"
  ) +
  theme_bw() +
  labs(x = "ICC", y = "Pearson's Coefficient") +
  coord_fixed()
```

# Data Checking on Correlation Matrix

```{r}
#| label: prepare-data

indices_wider <- indices_pooled |>
  anti_join(indicators_remove, by = c("game_name", "index_name")) |>
  filter(!need_trans | (index_type == "trans" & need_trans)) |>
  pivot_wider_indices(
    name_value = "score",
    always_suffix = TRUE
  ) |>
  left_join(
    users |>
      mutate(school = str_remove(school, "认知实验")) |>
      select(user_id, school),
    by = "user_id"
  )
indices_wider_bare <- select(indices_wider, -user_id, -school)
indices_wider_bare_sic <- indices_wider |>
  filter(school == "四川师范大学") |>
  select(-user_id, -school)
indices_wider_bare_bj <- indices_wider |>
  filter(school != "四川师范大学") |>
  select(-user_id, -school)
```

```{r}
#| label: fig-miss-pattern
#| column: page
#| fig-cap: Missing Pattern
#| fig-width: 15
#| fig-height: 9

naniar::vis_miss(indices_wider_bare) +
  scale_y_continuous(expand = c(0, 0), trans = "reverse") +
  theme(axis.text.x = element_text(angle = 90))
```

```{r}
#| label: fig-overall-correlation
#| column: page
#| fig-cap: Overview of Correlation Matrix
#| fig-subcap:
#|   - Whole Dataset
#|   - Sishuan Dataset
#|   - Beijing Dataset
#| fig-width: 15
#| fig-height: 15

vis_cor(indices_wider_bare, quiet = TRUE)
vis_cor(indices_wider_bare_sic, quiet = TRUE)
vis_cor(indices_wider_bare_bj, quiet = TRUE)
```

The correlation matrix of Beijing Data set is not positive definite, thus not appropriate for further analysis. Further check on the data shows there are only `r sum(complete.cases(indices_wider_bare_bj))` complete cases, which corrupts the data set. But the whole data set and Sichuan sub data set are all suitable for further analysis.

## Bartlett's Test

```{r}
#| label: tbl-bartlett-test
#| tbl-cap: Bartlett Test for Different Dataset

list(
  Whole = indices_wider_bare,
  `Sichuan Dataset` = indices_wider_bare_sic
) |>
  map(
    ~ psych::cortest.bartlett(.) |>
      as_tibble()
  ) |>
  bind_rows(.id = "Data Source") |>
  knitr::kable(digits = 2)
```

## Sampling Adequacy

KMO test: this test will give us an index of how well the matrix is suited for factor analysis. The criteria suggested by Kaiser [-@dziuban1974]:

![](images/paste-0D803EF7.png)

```{r}
#| label: KMO-result

kmo_result <- psych::KMO(indices_wider_bare)
kmo_result_sic <- psych::KMO(indices_wider_bare_sic)
bind_rows(
  whole = kmo_result$MSAi |>
    enframe(name = "game_index_name", value = "MSA"),
  sichuan = kmo_result_sic$MSAi |>
    enframe(name = "game_index_name", value = "MSA"),
  .id = "src"
) |>
  separate(game_index_name, c("game_name", "index_name"), sep = "\\.") |>
  mutate(MSA = digits(MSA, 2)) |>
  pivot_wider(
    names_from = src,
    values_from = MSA,
    names_prefix = "MSA_"
  ) |>
  formattable() |>
  as.datatable(caption = "KMO Test Result")
```

-   For the whole data set, the overall MSA value is `r round(kmo_result$MSA, 2)`. However, there are `r sum(kmo_result$MSAi < 0.6)` indices with MSA value lower than 0.6, which is generally acceptable.

-   For the Sichuan data set, the overall MSA value is `r round(kmo_result_sic$MSA, 2)`. However, there are `r sum(kmo_result_sic$MSAi < 0.6)` indices with MSA value lower than 0.6, which is generally acceptable.

# Exploratory Factor Analysis

## Determining Number of Factors

```{r}
#| label: calc-factors
#| fig-show: hide

n_factor_whole <- psych::nfactors(indices_wider_bare)
n_factor_sic <- psych::nfactors(indices_wider_bare_sic)
```

```{r}
#| label: tbl-nfactors
#| tbl-cap: Number of Factors Tests
#| tbl-subcap:
#|   - Whole Dataset
#|   - Sichuan Dataset

format_n_factors(n_factor_whole)
format_n_factors(n_factor_sic)
```

From @tbl-nfactors, we noticed inconsistencies. So we try factor numbers as 9, 6 and 3.

## Results of 9 factors

```{r}
#| label: fig-nine-factor-whole
#| fig-cap: Nine Factors Result (Whole Dataset)
#| column: page

fitted <- fa(indices_wider_bare, 9)
plotly_efa(fitted)
```

```{r}
#| label: nine-factor-whole
#| column: page
parameters::model_parameters(fitted, sort = TRUE, threshold = "max")
```

```{r}
#| label: fig-nine-factor-sic
#| fig-cap: Nine Factors Result (Sichuan Dataset)
#| column: page

fitted_sic <- fa(indices_wider_bare_sic, 9)
plotly_efa(fitted_sic)
```

```{r}
#| label: nine-factor-sic
#| column: page
parameters::model_parameters(fitted_sic, sort = TRUE, threshold = "max")
```

These two models have the least sample size adjusted BIC, but it captures many trivial factors with two to three task indices. Factors need at least three manifest variables to be stable or meaningful. But see @fig-overall-correlation, these factors are a reflection of those task pairs with a rather high correlation (about 0.5) with each other, both of which have relatively lower correlations with other tasks.

## Results of 6 factors

```{r}
#| label: fig-six-factor-whole
#| fig-cap: Six Factors Result (Whole Dataset)
#| column: page

fitted <- fa(indices_wider_bare, 6)
plotly_efa(fitted)
```

```{r}
#| label: six-factor-whole
#| column: page
parameters::model_parameters(fitted, sort = TRUE, threshold = "max")
```

```{r}
#| label: fig-six-factor-sic
#| fig-cap: Six Factors Result (Sichuan Dataset)
#| column: page

fitted_sic <- fa(indices_wider_bare_sic, 6)
plotly_efa(fitted_sic)
```

```{r}
#| label: six-factor-sic
#| column: page
parameters::model_parameters(fitted_sic, sort = TRUE, threshold = "max")
```

## Results of 3 factors

```{r}
#| label: fig-three-factor-whole
#| fig-cap: Three Factors Result (Whole Dataset)
#| column: page

fitted <- fa(indices_wider_bare, 3)
plotly_efa(fitted)
```

```{r}
#| label: three-factor-whole
#| column: page
parameters::model_parameters(fitted, sort = TRUE, threshold = "max")
```

```{r}
#| label: fig-three-factor-sic
#| fig-cap: Three Factors Result (Sichuan Dataset)
#| column: page

fitted_sic <- fa(indices_wider_bare_sic, 3)
plotly_efa(fitted_sic)
```

```{r}
#| label: three-factor-sic
#| column: page
parameters::model_parameters(fitted_sic, sort = TRUE, threshold = "max")
```

# Keep Variables of Low-reliability

In the following we try using all the variables regardless its test-retest reliability.

```{r}
#| label: prepare-data-full

indices_wider <- indices_pooled |>
  filter(!need_trans | (index_type == "trans" & need_trans)) |>
  pivot_wider_indices(
    name_value = "score",
    always_suffix = TRUE
  ) |>
  left_join(
    users |>
      mutate(school = str_remove(school, "认知实验")) |>
      select(user_id, school),
    by = "user_id"
  )
indices_wider_bare <- select(indices_wider, -user_id, -school)
indices_wider_bare_sic <- indices_wider |>
  filter(school == "四川师范大学") |>
  select(-user_id, -school)
indices_wider_bare_bj <- indices_wider |>
  filter(school != "四川师范大学") |>
  select(-user_id, -school)
```

## Correlation Matrix

```{r}
#| label: fig-miss-pattern-full
#| column: page
#| fig-cap: Missing Pattern
#| fig-width: 15
#| fig-height: 9

naniar::vis_miss(indices_wider_bare) +
  scale_y_continuous(expand = c(0, 0), trans = "reverse") +
  theme(axis.text.x = element_text(angle = 90))
```

```{r}
#| label: fig-overall-correlation-full
#| column: page
#| fig-cap: Overview of Correlation Matrix
#| fig-subcap:
#|   - Whole Dataset
#|   - Sishuan Dataset
#|   - Beijing Dataset
#| fig-width: 15
#| fig-height: 15

vis_cor(indices_wider_bare, quiet = TRUE)
vis_cor(indices_wider_bare_sic, quiet = TRUE)
vis_cor(indices_wider_bare_bj, quiet = TRUE)
```

Further checking on the correlation matrix of Beijing Data set shows it is not positive definite, either.

### Bartlett's Test

```{r}
#| label: tbl-bartlett-test-full
#| tbl-cap: Bartlett Test for Different Dataset

list(
  Whole = indices_wider_bare,
  `Sichuan Dataset` = indices_wider_bare_sic
) |>
  map(
    ~ psych::cortest.bartlett(.) |>
      as_tibble()
  ) |>
  bind_rows(.id = "Data Source") |>
  knitr::kable(digits = 2)
```

### Sampling Adequacy

```{r}
#| label: KMO-result-full

kmo_result <- psych::KMO(indices_wider_bare)
kmo_result_sic <- psych::KMO(indices_wider_bare_sic)
bind_rows(
  whole = kmo_result$MSAi |>
    enframe(name = "game_index_name", value = "MSA"),
  sichuan = kmo_result_sic$MSAi |>
    enframe(name = "game_index_name", value = "MSA"),
  .id = "src"
) |>
  separate(game_index_name, c("game_name", "index_name"), sep = "\\.") |>
  arrange(desc(MSA)) |> 
  mutate(MSA = digits(MSA, 2)) |>
  pivot_wider(
    names_from = src,
    values_from = MSA,
    names_prefix = "MSA_"
  ) |>
  formattable() |>
  as.datatable(caption = "KMO Test Result")
indices_wider_bare <- select(indices_wider_bare, -候鸟迁徙PRO.switch_cost_mrt)
```

One variable (switch cost for 候鸟迁徙) for the whole data set show too low MSA value (< 0.5). Further analysis does not include it, and further analysis will stick with the whole data set only.

## Determining Number of Factors

```{r}
#| label: calc-factors-full
#| fig-show: hide

n_factor_whole <- psych::nfactors(indices_wider_bare)
```

```{r}
#| label: tbl-nfactors-full
#| tbl-cap: Number of Factors Tests

format_n_factors(n_factor_whole)
```

## Nine Factor Model

```{r}
#| label: fig-nine-factor-whole-full
#| fig-cap: Nine Factors Result (Whole Dataset)
#| column: page

fitted <- fa(indices_wider_bare, 9)
plotly_efa(fitted)
```

```{r}
#| label: nine-factor-whole-full
#| column: page
parameters::model_parameters(fitted, sort = TRUE, threshold = "max")
```

## Seven Factor Model

```{r}
#| label: fig-seven-factor-whole-full
#| fig-cap: Seven Factors Result (Whole Dataset)
#| column: page

fitted <- fa(indices_wider_bare, 7)
plotly_efa(fitted)
```

```{r}
#| label: seven-factor-whole-full
#| column: page
parameters::model_parameters(fitted, sort = TRUE, threshold = "max")
```

## Three Factor Model

```{r}
#| label: fig-three-factor-whole-full
#| fig-cap: Three Factors Result (Whole Dataset)
#| column: page

fitted <- fa(indices_wider_bare, 3)
plotly_efa(fitted)
```

```{r}
#| label: three-factor-whole-full
#| column: page
parameters::model_parameters(fitted, sort = TRUE, threshold = "max")
```

# Additional Analysis

One might be interested which tasks will have the highest loading if we fit only one latent factor. Here is the result (note the variable with low MSA value is still removed):

```{r}
#| label: fig-one-factor-loading
#| fig-cap: Loadings on One Factor Model
#| fig-height: 12
#| fig-width: 10

model_one_fac <- fa(indices_wider_bare, 1)
model_one_fac$loadings |> 
  unclass() |> 
  as_tibble(rownames = "variable") |> 
  mutate(variable = fct_reorder(variable, MR1)) |>  
  ggplot(aes(variable, MR1)) +
  geom_col() +
  coord_flip() +
  theme_bw() +
  labs(y = "Loading") +
  theme(axis.title.y = element_blank())
```
