---
title: Explore Cognitive Structure
author: Liang Zhang
date: 2022-08-21
format:
  html:
    code-fold: true
    toc: true
    toc-location: left
category: structure
execute:
  warning: false
  showtext: true
bibliography: references.bib
---

```{r}
#| label: setup

library(tidyverse)
library(corrr)
library(showtext)
library(gt)
library(formattable)
library(patchwork)
library(psych)
requireNamespace("bit64")
```

```{r}
#| label: utils

pivot_wider_indices <- function(data, name_value = "test", always_suffix = FALSE) {
  data |>
    group_by(game_name) |>
    mutate(n_index = n_distinct(index_name)) |>
    ungroup() |>
    mutate(
      game_index = if (always_suffix) {
        str_c(game_name, index_name, sep = ".")
      } else {
        if_else(
          n_index == 1,
          game_name,
          str_c(game_name, index_name, sep = ".")
        )
      }
    ) |>
    pivot_wider(
      id_cols = user_id,
      names_from = game_index,
      values_from = .data[[name_value]]
    )
}

summary_data <- function(data, name_value = "test") {
  pivot_wider_indices(data, name_value, always_suffix = TRUE) |>
    select(-user_id) |>
    psych::describe() |>
    as_tibble(rownames = "game_index_name") |>
    select(game_index_name, n, mean, sd, min, max, skew, kurtosis) |>
    separate(game_index_name, c("game_name", "index_name"), sep = "\\.") |>
    mutate(
      skew_status = case_when(
        abs(skew) > 1 ~ "highly",
        abs(skew) > 0.5 ~ "moderately",
        TRUE ~ "acceptable"
      )
    )
}

show_normality <- function(data, game_name, index_name, skew, ...,
                           name_value = "test") {
  p_hist <- ggplot(data, aes_string(name_value)) +
    geom_histogram(bins = 30) +
    labs(x = "Raw Score", y = "Count") +
    theme_bw()
  p_qq <- ggplot(data, aes_string(sample = name_value)) +
    geom_qq() +
    geom_qq_line() +
    labs(x = "Expected Quantile", y = "Observed Quantile") +
    theme_bw()
  wrap_elements(grid::textGrob(
    str_glue("{game_name}\n{index_name}\nskewness = {round(skew, 2)}"),
    rot = 90
  )) + p_hist + p_qq +
    plot_layout(widths = c(0.1, 0.45, 0.45))
}

transform_indices <- function(x, game_name, index_name) {
  if (game_name == "小狗回家" & index_name == "mean_score") {
    return(-log10(1 - x))
  }
  if (game_name == "各得其所" & index_name == "mrt_init") {
    return(log10(x))
  }
  if (game_name == "数字推理") {
    return(sqrt(x))
  }
  if (game_name == "塔罗牌") {
    return(-sqrt(max(x, na.rm = TRUE) - x))
  }
  if (game_name == "人工语言-中级") {
    return(-sqrt(max(x, na.rm = TRUE) - x))
  }
  if (game_name == "语义判断") {
    return(-log10(max(x, na.rm = TRUE) + 1 - x))
  }
  if (game_name == "数感") {
    return(log10(x))
  }
  if (game_name == "时长分辨") {
    return(log10(x))
  }
  if (game_name == "节奏感知") {
   return(sqrt(x))
  }
  return(x)
}

calc_test_retest <- function(data,
                             name_test = "test",
                             name_retest = "retest") {
  data_no_missing <- data |>
    select(all_of(c(name_test, name_retest))) |>
    drop_na()
  outliers_result <- data_no_missing |>
    performance::check_outliers(method = "mahalanobis_robust")
  data_no_outlier <- data_no_missing |>
    slice(-which(outliers_result))
  icc_result <- ICC(data_no_outlier)
  data.frame(
    n_obs = icc_result$n.obs,
    icc = icc_result$results$ICC[[2]],
    r = cor(data_no_outlier[[name_test]], data_no_outlier[[name_retest]])
  )
}
```

```{r}
#| label: load-data

targets::tar_load(users)
targets::tar_load(indices_selection)
targets::tar_load(indices_struct)
dim_order <- c(
  "attention", "multitask", "switching", "inhibition",
  "reasoning", "complex span", "working memory", "short term memory",
  "long term memory", "probability learning",
  "speeded IP", "strategic IP",
  "perception", "math", "language"
)
indices <- indices_struct |>
  arrange(factor(dimension, dim_order)) |>
  group_by(game_name, index_name) |>
  # remove 1d outliers based on "iqr"
  filter(!test %in% boxplot.stats(test)$out) |>
  ungroup()
indices_stats <- summary_data(indices)
```

# Data Checking on Normality

It is necessary to check normality before performing exploratory analysis.

## Original Form

```{r}
#| label: fig-histograms
#| column: page
#| fig-cap: Histogram of All Game Indices
#| fig-width: 10
#| fig-height: 18

indices |>
  mutate(facet_title = str_c(game_name, index_name, sep = "\n")) |>
  ggplot(aes(test)) +
  geom_histogram(bins = 30) +
  facet_wrap(~facet_title, scales = "free", ncol = 5) +
  labs(x = "Raw Score", y = "Count") +
  theme_bw()
```

```{r}
#| label: fig-qqplot
#| column: page
#| fig-cap: QQ-plot of All Game Indices
#| fig-width: 10
#| fig-height: 18

indices |>
  mutate(facet_title = str_c(game_name, index_name, sep = "\n")) |>
  ggplot(aes(sample = test)) +
  geom_qq() +
  geom_qq_line() +
  facet_wrap(~facet_title, scales = "free", ncol = 5) +
  labs(x = "Expected Quantile", y = "Observed Quantile") +
  theme_bw()
```

According to a classic book [@kline2016], critical value for absolute skewness is 3, and critical value for absolute kurtosis is 10; thus, these indices are all not too apart from normal distribution. But see @fig-histograms, we detect some heavily skewed indices. Just as a classic book [@bulmer1979] said, critical value for skewness is 0.5 (moderately skew) and 1 (highly skew).

```{r}
#| label: data-description
#| column: page

indices_stats |>
  mutate(across(mean:kurtosis, ~ digits(., 3))) |>
  formattable() |>
  as.datatable(
    caption = "Descriptive Statistics of All Indices"
  )
```

## Transformed

Here we decide to convert the moderately and highly indices based on the conversion suggested by Howell [-@howell2013].

```{r}
#| label: fig-normality-skewed
#| fig-cap: Normality Check on Suspected Skew Indices
#| fig-width: 8
#| fig-height: 18

indices |>
  inner_join(
    indices_stats |>
      select(game_name, index_name, skew, skew_status) |>
      filter(skew_status != "acceptable"),
    by = c("game_name", "index_name")
  ) |>
  group_nest(game_name, index_name, skew, skew_status) |>
  pmap(show_normality) |>
  wrap_plots(ncol = 1)
```

From @fig-normality-skewed, it might be a good choice to transform indices with an absolute skewness value larger than 0.5, and logarithmic and square root transformations are used.

```{r}
#| label: transform-data

game_needs_trans <- indices_stats |>
  filter(abs(skew) > 0.5)
indices_trans <- indices |>
  semi_join(game_needs_trans, by = c("game_name", "index_name")) |>
  group_by(game_name, index_name) |>
  group_modify(
    ~ .x |>
      mutate(
        across(
          c(test, retest),
          transform_indices,
          game_name = .y$game_name,
          index_name = .y$index_name
        )
      )
  ) |>
  ungroup() |>
  mutate(score = if_else(reversed == "yes", -test, test))
indices_stats_trans <- summary_data(indices_trans)
indices_pooled <- bind_rows(
  origin = indices,
  trans = indices_trans,
  .id = "index_type"
) |>
  left_join(
    game_needs_trans |>
      add_column(need_trans = TRUE) |>
      select(game_name, index_name, need_trans),
    by = c("game_name", "index_name")
  ) |>
  mutate(need_trans = coalesce(need_trans, FALSE))
```

```{r}
#| label: fig-normality-trans
#| fig-cap: Normality Check after Transformations
#| fig-width: 8
#| fig-height: 18

indices_trans |>
  inner_join(indices_stats_trans, by = c("game_name", "index_name")) |>
  group_nest(game_name, index_name, skew, skew_status) |>
  pmap(show_normality) |>
  wrap_plots(ncol = 1)
```

From @fig-normality-trans, we know these indices now perform very well on normality. The transformations seem to be useful.

# Data Checking on Reliability

```{r}
#| label: reliability-list
#| column: page

test_retest <- indices_pooled |>
  group_by(game_name, index_name, dimension, index_type, need_trans) |>
  group_modify(~ calc_test_retest(.)) |>
  ungroup()
format_na <- function(str, x) {
  ifelse(is.na(x), "-", str)
}
test_retest |>
  pivot_wider(
    id_cols = c(game_name, index_name),
    names_from = index_type,
    values_from = c(n_obs, icc, r),
    names_vary = "slowest"
  ) |>
  mutate(
    across(
      starts_with(c("icc", "r")),
      ~ digits(., 2, postproc = format_na)
    ),
    n_obs_trans = digits(n_obs_trans, 0, postproc = format_na)
  ) |>
  formattable(
    list(
      formattable::area(col = c(icc_origin, icc_trans)) ~
        color_tile("white", "pink", na.rm = TRUE),
      game_name = formatter(
        "span",
        style = ~ if_else(
          coalesce(icc_trans, icc_origin) < 0.5,
          "background:grey",
          NA_character_
        )
      )
    )
  ) |>
  as.datatable()
```

From the above table, it shows that the transformations are successful in elevating test-retest reliability. Kline stated in their book [-@applied2013]: "If $r_{xx} < .50$, then most of the total variance is due to measurement error. Indicators with such low score reliabilities should be excluded from the analysis." We should remove those indices with test-retest reliability less than 0.5 (grey background cells) according to this quote. The removed indicators are:

```{r}
#| label: indicators-removed

indicators_remove <- test_retest |>
  filter(!need_trans | (index_type == "trans" & need_trans)) |>
  filter(icc < 0.5) |>
  select(game_name, index_name, dimension, icc)
indicators_remove |>
  knitr::kable(
    digits = 2,
    caption = "Removed Indicators",
    row.names = TRUE
  )
```

Note there is a very special case, i.e. "我是大厨", whose pearson correlation coefficient is substantially larger than ICC value. Further analysis (see @fig-cook-test-retest) shows that participants improve more if they perform worse in the first test. In this case, it makes us doubt whether intraclass correlation (ICC) is suitable for cognitive ability tests for there is always practice effect (they do no really use parallel versions).

```{r}
#| label: fig-cook-test-retest
#| fig-cap: Special Case for Test-retest Reliability
#| fig-width: 6
#| fig-height: 6

indices_pooled |>
  filter(game_name == "我是大厨") |>
  ggplot(aes(test, retest)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x) +
  geom_abline(
    slope = 1,
    intercept = 0,
    linetype = "dotted",
    color = "lightblue"
  ) +
  theme_bw() +
  coord_fixed() +
  labs(x = "Test Phase", y = "Retest Phase")
```

But see @fig-scatter-icc-pearson, these two reliability indices are closely correlated with each other.

```{r}
#| label: fig-scatter-icc-pearson
#| fig-cap: Correlation Between ICC and Pearson's Coefficient
#| fig-height: 6
#| fig-width: 6

test_retest |>
  ggplot(aes(icc, r)) +
  geom_point() +
  stat_smooth(method = "lm", formula = y ~ x) +
  ggpmisc::stat_correlation(small.r = TRUE) +
  geom_abline(
    slope = 1,
    intercept = 0,
    linetype = "dotted",
    color = "lightblue"
  ) +
  theme_bw() +
  labs(x = "ICC", y = "Pearson's Coefficient") +
  coord_fixed()
```

# Data Checking on Correlation Matrix

```{r}
#| label: prepare-data

indices_wider <- indices_pooled |>
  anti_join(indicators_remove, by = c("game_name", "index_name")) |>
  filter(!need_trans | (index_type == "trans" & need_trans)) |>
  pivot_wider_indices(
    name_value = "score",
    always_suffix = TRUE
  ) |>
  left_join(
    users |>
      mutate(school = str_remove(school, "认知实验")) |>
      select(user_id, school),
    by = "user_id"
  )
indices_wider_bare <- select(indices_wider, -user_id, -school)
indices_wider_bare_sic <- indices_wider |>
  filter(school == "四川师范大学") |>
  select(-user_id, -school)
indices_wider_bare_bj <- indices_wider |>
  filter(school != "四川师范大学") |>
  select(-user_id, -school)
```

```{r}
#| label: fig-miss-pattern
#| column: page
#| fig-cap: Missing Pattern
#| fig-width: 15
#| fig-height: 9

naniar::vis_miss(indices_wider_bare) +
  scale_y_continuous(expand = c(0, 0), trans = "reverse") +
  theme(axis.text.x = element_text(angle = 90))
```

```{r}
#| label: fig-overall-correlation
#| column: page
#| fig-cap: Overview of Correlation Matrix
#| fig-subcap:
#|   - Whole Dataset
#|   - Sishuan Dataset
#|   - Beijing Dataset
#| fig-width: 15
#| fig-height: 15

indices_wider_bare |>
  correlate(quiet = TRUE) |>
  autoplot(method = "HC", triangular = "full") +
  scico::scale_fill_scico(palette = "bam", midpoint = 0, direction = -1)
indices_wider_bare_sic |>
  correlate(quiet = TRUE) |>
  autoplot(method = "HC", triangular = "full") +
  scico::scale_fill_scico(palette = "bam", midpoint = 0, direction = -1)
indices_wider_bare_bj |>
  correlate(quiet = TRUE) |>
  autoplot(method = "HC", triangular = "full") +
  scico::scale_fill_scico(palette = "bam", midpoint = 0, direction = -1)
```

## Bartlett's Test

```{r}
#| label: tbl-bartlett-test
#| tbl-cap: Bartlett Test for Different Dataset

list(
  Whole = indices_wider_bare,
  `Sichuan Dataset` = indices_wider_bare_sic,
  `Beijing Dataset` = indices_wider_bare_bj
) |>
  map(
    ~ psych::cortest.bartlett(.) |>
      as_tibble()
  ) |>
  bind_rows(.id = "Data Source") |>
  knitr::kable(digits = 2)
```

@tbl-bartlett-test shows that Beijing data set is not appropriate for further analysis, because the correlation matrix is not positive definite. Further check on the data shows there is only `r sum(complete.cases(indices_wider_bare_bj))` complete cases, which corrupts the data set. But the whole data set and Sichuan sub data set are all suitable for further analysis.

## Sampling Adequacy

KMO test: this test will give us an index of how well the matrix is suited for factor analysis. The criteria suggested by Kaiser [-@dziuban1974]:

![](images/paste-0D803EF7.png)

```{r}
#| label: KMO-result

kmo_result <- psych::KMO(indices_wider_bare)
kmo_result_sic <- psych::KMO(indices_wider_bare_sic)
bind_rows(
  whole = kmo_result$MSAi |>
    enframe(name = "game_index_name", value = "MSA"),
  sichuan = kmo_result_sic$MSAi |>
    enframe(name = "game_index_name", value = "MSA"),
  .id = "src"
) |>
  separate(game_index_name, c("game_name", "index_name"), sep = "\\.") |>
  mutate(MSA = digits(MSA, 2)) |>
  pivot_wider(
    names_from = src,
    values_from = MSA,
    names_prefix = "MSA_"
  ) |>
  formattable() |>
  as.datatable(caption = "KMO Test Result")
```

-   For the whole data set, the overall MSA value is `r round(kmo_result$MSA, 2)`. However, there are `r sum(kmo_result$MSAi < 0.6)` indices with MSA value lower than 0.6, which is generally acceptable.

-   For the Sichuan data set, the overall MSA value is `r round(kmo_result_sic$MSA, 2)`. However, there are `r sum(kmo_result_sic$MSAi < 0.6)` indices with MSA value lower than 0.6, which is generally acceptable.

# Exploratory Factor Analysis

## Determining Number of Factors

```{r}
#| label: calc-factors
#| fig-show: hide

n_factor_whole <- psych::nfactors(indices_wider_bare)
n_factor_sic <- psych::nfactors(indices_wider_bare_sic)
```

```{r}
#| label: tbl-nfactors
#| tbl-cap: Number of Factors Tests
#| tbl-subcap:
#|   - Whole Dataset
#|   - Sichuan Dataset

format_n_factors <- function(n_factor_result) {
  checks <- c("BIC", "SABIC", "eBIC", "MAP")
  n_factor_result$vss.stats |>
    as_tibble() |>
    select(BIC, SABIC, eBIC) |>
    mutate(n_factor = seq_len(n()), .before = 1L) |>
    add_column(MAP = n_factor_result$map) |>
    gt() |>
    fmt_number(
      -c(n_factor, MAP),
      decimals = 0
    ) |>
    fmt_scientific(MAP) |>
    tab_style(
      style = cell_text(weight = "bold"),
      locations = map(
        checks,
        ~ cells_body(
          columns = all_of(.x),
          rows = !!sym(.x) == min(!!sym(.x))
        )
      )
    ) |>
    as_raw_html()
}

format_n_factors(n_factor_whole)
format_n_factors(n_factor_sic)
```

From @tbl-nfactors, we noticed inconsistencies. So we try factor numbers as 9, 6 and 3.

```{r}
#| label: utils-efa

plotly_efa <- function(model) {
  heatmaply::heatmaply_cor(
    t(unclass(model$loadings)),
    dendrogram = "column",
    k_col = NA,
    label_format_fun = function(...) round(..., digits = 2),
    margin = c(50, 50, 50, 0)
  )
}
```

## Results of 9 factors

```{r}
#| label: fig-nine-factor-whole
#| fig-cap: Nine Factors Result (Whole Dataset)
#| column: page

fitted <- fa(indices_wider_bare, 9)
plotly_efa(fitted)
```

```{r}
#| label: fig-nine-factor-sic
#| fig-cap: Nine Factors Result (Sichuan Dataset)
#| column: page

fitted_sic <- fa(indices_wider_bare_sic, 9)
plotly_efa(fitted_sic)
```

## Results of 6 factors

```{r}
#| label: fig-six-factor-whole
#| fig-cap: Six Factors Result (Whole Dataset)
#| column: page

fitted <- fa(indices_wider_bare, 6)
plotly_efa(fitted)
```

```{r}
#| label: fig-six-factor-sic
#| fig-cap: Six Factors Result (Sichuan Dataset)
#| column: page

fitted_sic <- fa(indices_wider_bare_sic, 6)
plotly_efa(fitted_sic)
```

```{r}
#| label: six-factor-varimin
#| column: page
#| eval: false

fitted <- fa(indices_wider_bare, 7, rotate = "varimin")
parameters::model_parameters(fitted, sort = TRUE, threshold = 0.3)
```

## Results of 3 factors

```{r}
#| label: fig-three-factor-whole
#| fig-cap: Three Factors Result (Whole Dataset)
#| column: page

fitted <- fa(indices_wider_bare, 3)
plotly_efa(fitted)
```

```{r}
#| label: fig-three-factor-sic
#| fig-cap: Three Factors Result (Sichuan Dataset)
#| column: page

fitted_sic <- fa(indices_wider_bare_sic, 3)
plotly_efa(fitted_sic)
```
