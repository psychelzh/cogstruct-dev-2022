[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "We are from Beijing Normal University. We have developed more than 50 tasks based on psychological paradigms, in order to explore the structure of human cognition."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cognitive Structure Project",
    "section": "",
    "text": "Jun 8, 2022\n\n\nLiang Zhang, Yifei Cao\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstructure\n\n\ncognition\n\n\nmeta-analysis\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2022\n\n\nYifei Cao, Liang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstructure\n\n\nwm\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2022\n\n\nLiang Zhang\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata-check\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2022\n\n\nLiang Zhang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/check-raw-data/index.html",
    "href": "posts/check-raw-data/index.html",
    "title": "Check Raw Data",
    "section": "",
    "text": "Here we check raw data from several special tasks. Especially check the factors influencing reliability, internal consistency of each task"
  },
  {
    "objectID": "posts/check-raw-data/index.html#forward-word-span-过目不忘",
    "href": "posts/check-raw-data/index.html#forward-word-span-过目不忘",
    "title": "Check Raw Data",
    "section": "Forward Word Span (过目不忘)",
    "text": "Forward Word Span (过目不忘)\n\n\nCode\ntargets::tar_load(data_valid_FWSPro, store = here::here(\"preproc/_targets\"))\nchrs_freq <- read_tsv(\"CharFreq.txt\", skip = 5)\nchrs_used <- readxl::read_excel(\"过目不忘-汉字库.xlsx\") |>\n  left_join(chrs_freq, by = \"汉字\")\ndata_adj_acc <- data_valid_FWSPro |>\n  unnest(raw_parsed) |>\n  group_by(user_id, game_time) |>\n  mutate(trial = row_number()) |>\n  ungroup() |>\n  mutate(\n    across(\n      c(stim, resp),\n      str_split,\n      pattern = \"-\"\n    )\n  ) |>\n  unnest(c(stim, resp)) |>\n  left_join(\n    select(chrs_used, stim = 汉字, stim_id = ID, stim_freq = 序列号),\n    by = \"stim\"\n  ) |>\n  left_join(\n    select(chrs_used, resp = 汉字, resp_id = ID),\n    by = \"resp\"\n  ) |>\n  separate(stim_id, c(\"stim_phon\", \"stim_form\"), convert = TRUE) |>\n  separate(resp_id, c(\"resp_phon\", \"resp_form\"), convert = TRUE) |>\n  mutate(\n    acc = case_when(\n      stim == resp ~ \"正确\",\n      stim_phon == resp_phon ~ \"同音字\",\n      stim_phon != resp_phon ~ \"错误\"\n    )\n  )\ndata_adj_acc |>\n  group_by(stim, stim_phon, stim_freq, acc) |>\n  summarise(n = n(), .groups = \"drop_last\") |>\n  mutate(prop = n / sum(n)) |>\n  ungroup() |>\n  ggplot(aes(stim, prop, fill = acc)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(\n    aes(label = scales::label_percent(accuracy = 1)(prop)),\n    position = position_stack(vjust = 0.5),\n    color = \"white\"\n  ) +\n  geom_text(\n    aes(label = stim_freq),\n    y = 0\n  ) +\n  scale_fill_brewer(palette = \"Accent\") +\n  facet_wrap(~ stim_phon, scales = \"free_x\", nrow = 1) +\n  labs(x = \"\", y = \"\", fill = \"\") +\n  # scale_y_continuous(expand = c(0, 0)) +\n  ggthemes::theme_hc() +\n  theme(\n    axis.text.x = element_text(family = \"SimHei\"),\n    legend.text = element_text(family = \"SimHei\")\n  )\n\n\n\n\n\nHomophone Selection Proportion"
  },
  {
    "objectID": "posts/check-raw-data/index.html#schulte-grid-舒尔特方格",
    "href": "posts/check-raw-data/index.html#schulte-grid-舒尔特方格",
    "title": "Check Raw Data",
    "section": "Schulte Grid (舒尔特方格)",
    "text": "Schulte Grid (舒尔特方格)\n\n\nCode\ntargets::tar_load(data_valid_SchulteMed, store = here::here(\"preproc/_targets\"))\nrt_by_resp <- data_valid_SchulteMed |>\n  mutate(\n    raw_parsed = map(\n      raw_parsed,\n      ~ . |> mutate(resp = as.integer(resp))\n    )\n  ) |>\n  unnest(raw_parsed) |>\n  group_by(user_id, game_time) |>\n  mutate(resp_adj = ifelse(acc == 0, NA, resp)) |>\n  fill(resp_adj, .direction = \"up\") |>\n  ungroup() |>\n  drop_na() |>\n  group_by(user_id, game_version, game_time, resp_adj) |>\n  summarise(rt = sum(rt) / 1000, .groups = \"drop\") |>\n  filter(rt < 300)\nrt_by_resp |>\n  ggplot(aes(resp_adj, rt, color = game_version)) +\n  geom_point(shape = \".\") +\n  geom_smooth() +\n  scale_y_log10() +\n  scale_color_brewer(palette = \"Paired\") +\n  labs(x = \"\", y = \"Response Time (s)\", color = \"Version\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/check-raw-data/index.html#reasoning-推理类题目",
    "href": "posts/check-raw-data/index.html#reasoning-推理类题目",
    "title": "Check Raw Data",
    "section": "Reasoning (推理类题目)",
    "text": "Reasoning (推理类题目)\n\n\nCode\ntargets::tar_load(data_valid_DRA, store = here::here(\"preproc/_targets\"))\ndata_valid_DRA |>\n  filter(course_name == \"清华大学认知实验D\") |>\n  unnest(raw_parsed) |>\n  mutate(item = as.numeric(as_factor(itemid))) |>\n  # group_by(item) |>\n  # filter(between(mean(acc == 1), 0.6, 0.9)) |>\n  # ungroup() |>\n  filter(acc != -1) |>\n  pivot_wider(\n    id_cols = user_id,\n    names_from = item,\n    values_from = acc\n  ) |>\n  select(-user_id)"
  },
  {
    "objectID": "posts/check-raw-data/index.html#neuroracer-小狗回家",
    "href": "posts/check-raw-data/index.html#neuroracer-小狗回家",
    "title": "Check Raw Data",
    "section": "NeuroRacer (小狗回家)",
    "text": "NeuroRacer (小狗回家)\n\n\nCode\ndata_racer_new <- targets::tar_read(\n  data_valid_Racer, \n  store = here::here(\"preproc/_targets\")\n) |> \n  tidyr::unnest(raw_parsed) |> \n  dplyr::mutate(block = paste0(\"V\", block)) |> \n  dplyr::filter(block != \"V0\")\ndata_racer_new |> \n  dplyr::group_by(user_id, block) |> \n  dplyr::summarise(\n    mean_score = sum(escortscore * trialdur) / sum(trialdur),\n    .groups = \"drop\"\n  ) |> \n  tidyr::pivot_wider(\n    id_cols = user_id, \n    names_from = block, \n    values_from = mean_score\n  ) |> \n  dplyr::select(-user_id) |> \n  psych::alpha()\n\n\n\nReliability analysis   \nCall: psych::alpha(x = dplyr::select(tidyr::pivot_wider(dplyr::summarise(dplyr::group_by(data_racer_new, \n    user_id, block), mean_score = sum(escortscore * trialdur)/sum(trialdur), \n    .groups = \"drop\"), id_cols = user_id, names_from = block, \n    values_from = mean_score), -user_id))\n\n  raw_alpha std.alpha G6(smc) average_r S/N   ase mean    sd median_r\n      0.85      0.85     0.8      0.66 5.7 0.045 0.84 0.043     0.62\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.73  0.85  0.92\nDuhachek  0.76  0.85  0.94\n\n Reliability if an item is dropped:\n   raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r\nV2      0.75      0.76    0.61      0.61 3.2    0.084    NA  0.61\nV3      0.77      0.77    0.62      0.62 3.3    0.081    NA  0.62\nV1      0.84      0.85    0.73      0.73 5.5    0.054    NA  0.73\n\n Item statistics \n    n raw.r std.r r.cor r.drop mean    sd\nV2 33  0.91  0.89  0.83   0.76 0.84 0.045\nV3 33  0.90  0.89  0.82   0.75 0.85 0.055\nV1 30  0.83  0.85  0.71   0.66 0.83 0.045\n\n\nCode\ndata_racer_new |> \n  dplyr::group_by(user_id, block) |> \n  dplyr::group_modify(\n    ~ preproc.iquizoo::cpt(.x)\n  )|> \n  dplyr::ungroup() |> \n  tidyr::pivot_wider(\n    id_cols = user_id, \n    names_from = block, \n    values_from = dprime\n  ) |> \n  dplyr::select(-user_id) |> \n  psych::alpha()\n\n\n\nReliability analysis   \nCall: psych::alpha(x = dplyr::select(tidyr::pivot_wider(dplyr::ungroup(dplyr::group_modify(dplyr::group_by(data_racer_new, \n    user_id, block), ~preproc.iquizoo::cpt(.x))), id_cols = user_id, \n    names_from = block, values_from = dprime), -user_id))\n\n  raw_alpha std.alpha G6(smc) average_r S/N  ase mean   sd median_r\n      0.52      0.53    0.45      0.27 1.1 0.15  2.3 0.55     0.23\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.14  0.52  0.75\nDuhachek  0.23  0.52  0.81\n\n Reliability if an item is dropped:\n   raw_alpha std.alpha G6(smc) average_r  S/N alpha se var.r med.r\nV2      0.37      0.38    0.23      0.23 0.60     0.22    NA  0.23\nV3      0.27      0.27    0.15      0.15 0.36     0.25    NA  0.15\nV1      0.59      0.60    0.42      0.42 1.48     0.14    NA  0.42\n\n Item statistics \n    n raw.r std.r r.cor r.drop mean   sd\nV2 33  0.77  0.73  0.53   0.36  2.4 0.82\nV3 33  0.75  0.77  0.60   0.44  2.3 0.69\nV1 30  0.64  0.64  0.31   0.22  2.1 0.78\n\n\n\n\nCode\ndata_racer_old <- targets::tar_read(\n  data_parsed_Racer, \n  store = here::here(\"preproc/_targets\")\n) |> \n  dplyr::filter(game_version == \"1.0.0\") |> \n  tidyr::unnest(raw_parsed) |> \n  dplyr::mutate(block = paste0(\"V\", block))\ndata_racer_old |> \n  dplyr::group_by(user_id, block) |> \n  dplyr::summarise(\n    mean_score = sum(produr * trialdur) / sum(trialdur),\n    .groups = \"drop\"\n  ) |> \n  tidyr::pivot_wider(\n    id_cols = user_id, \n    names_from = block, \n    values_from = mean_score\n  ) |> \n  dplyr::select(-user_id) |> \n  psych::alpha()\n\n\n\nReliability analysis   \nCall: psych::alpha(x = dplyr::select(tidyr::pivot_wider(dplyr::summarise(dplyr::group_by(data_racer_old, \n    user_id, block), mean_score = sum(produr * trialdur)/sum(trialdur), \n    .groups = \"drop\"), id_cols = user_id, names_from = block, \n    values_from = mean_score), -user_id))\n\n  raw_alpha std.alpha G6(smc) average_r  S/N   ase mean sd median_r\n     0.098      0.13    0.11     0.028 0.14 0.073  221 80    0.018\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt    -0.06   0.1  0.24\nDuhachek -0.05   0.1  0.24\n\n Reliability if an item is dropped:\n   raw_alpha std.alpha G6(smc) average_r   S/N alpha se   var.r    med.r\nV1     0.162     0.161   0.130     0.046 0.192    0.072 0.00177  0.03686\nV2     0.075     0.099   0.079     0.027 0.110    0.077 0.00132  0.01817\nV3     0.079     0.106   0.085     0.029 0.118    0.076 0.00137  0.01817\nV4     0.028     0.039   0.031     0.010 0.041    0.079 0.00055  0.00446\nV5     0.073     0.104   0.087     0.028 0.116    0.076 0.00278 -0.00002\n\n Item statistics \n     n raw.r std.r  r.cor r.drop mean  sd\nV1 363  0.62  0.43 0.0067 0.0036  310 246\nV2 363  0.38  0.47 0.1675 0.0486  197 132\nV3 363  0.39  0.47 0.1501 0.0430  192 141\nV4 363  0.47  0.52 0.3053 0.0882  200 158\nV5 363  0.43  0.47 0.1460 0.0482  205 155\n\n\nCode\ndata_racer_old |> \n  dplyr::group_by(user_id, block) |> \n  dplyr::group_modify(\n    ~ preproc.iquizoo::cpt(.x)\n  )|> \n  dplyr::ungroup() |> \n  tidyr::pivot_wider(\n    id_cols = user_id, \n    names_from = block, \n    values_from = dprime\n  ) |> \n  dplyr::select(-user_id) |> \n  psych::alpha()\n\n\n\nReliability analysis   \nCall: psych::alpha(x = dplyr::select(tidyr::pivot_wider(dplyr::ungroup(dplyr::group_modify(dplyr::group_by(data_racer_old, \n    user_id, block), ~preproc.iquizoo::cpt(.x))), id_cols = user_id, \n    names_from = block, values_from = dprime), -user_id))\n\n  raw_alpha std.alpha G6(smc) average_r S/N  ase mean  sd median_r\n      0.76      0.76    0.72      0.39 3.2 0.02  2.2 0.6     0.41\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.72  0.76   0.8\nDuhachek  0.72  0.76   0.8\n\n Reliability if an item is dropped:\n   raw_alpha std.alpha G6(smc) average_r S/N alpha se  var.r med.r\nV1      0.73      0.73    0.67      0.40 2.7    0.023 0.0010  0.42\nV2      0.70      0.70    0.64      0.37 2.3    0.026 0.0037  0.38\nV3      0.71      0.71    0.66      0.38 2.4    0.025 0.0053  0.39\nV4      0.72      0.72    0.67      0.39 2.6    0.024 0.0036  0.41\nV5      0.72      0.72    0.67      0.39 2.6    0.024 0.0035  0.41\n\n Item statistics \n     n raw.r std.r r.cor r.drop mean   sd\nV1 363  0.70  0.69  0.57   0.49  1.6 0.88\nV2 363  0.76  0.75  0.66   0.58  2.2 0.91\nV3 363  0.73  0.73  0.63   0.55  2.4 0.83\nV4 363  0.69  0.70  0.59   0.51  2.4 0.80\nV5 363  0.68  0.70  0.59   0.51  2.5 0.76"
  },
  {
    "objectID": "posts/explore-struct-wm/index.html",
    "href": "posts/explore-struct-wm/index.html",
    "title": "Explore Cognitive Structure for Working Memory",
    "section": "",
    "text": "Code\npivot_wider_indices <- function(data) {\n  data |>\n    add_count(user_id, game_name) |>\n    mutate(\n      game_index = if_else(\n        n == 1,\n        game_name,\n        str_c(game_name, index_name, sep = \"-\")\n      )\n    ) |>\n    pivot_wider(\n      id_cols = user_id,\n      names_from = game_index,\n      values_from = test\n    )\n}\nThe include tasks:"
  },
  {
    "objectID": "posts/explore-struct-wm/index.html#correlation-matrix",
    "href": "posts/explore-struct-wm/index.html#correlation-matrix",
    "title": "Explore Cognitive Structure for Working Memory",
    "section": "Correlation Matrix",
    "text": "Correlation Matrix\n\n\nCode\ncorrelate(indices_selected, quiet = TRUE) |> \n  rearrange(method = \"HC\") |>\n  stretch() |> \n  mutate(across(c(x, y), as_factor)) |>\n  ggplot(aes(x, y)) +\n  geom_tile(aes(fill = r)) +\n  scico::scale_fill_scico(palette = \"bam\", midpoint = 0, direction = -1) +\n  coord_fixed() +\n  theme_minimal(base_size = 18) +\n  labs(x = \"\", y = \"\", fill = \"Pearson's\", color = \"\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "posts/explore-struct-wm/index.html#multidimensional-scaling",
    "href": "posts/explore-struct-wm/index.html#multidimensional-scaling",
    "title": "Explore Cognitive Structure for Working Memory",
    "section": "Multidimensional Scaling",
    "text": "Multidimensional Scaling\n\n\nCode\nmds <- indices_selected |> \n  cor(use = \"pairwise\") |> \n  smacof::sim2diss(to.dist = TRUE) |> \n  smacof::mds(ndim = 2, type = \"mspline\")\n\n\nRegistered S3 methods overwritten by 'proxy':\n  method               from    \n  print.registry_field registry\n  print.registry_entry registry\n\n\nCode\nplot(mds)"
  },
  {
    "objectID": "posts/explore-struct-wm/index.html#exploratory-factor-analysis",
    "href": "posts/explore-struct-wm/index.html#exploratory-factor-analysis",
    "title": "Explore Cognitive Structure for Working Memory",
    "section": "Exploratory Factor Analysis",
    "text": "Exploratory Factor Analysis\n\nTraditional\n\n\nCode\nnfactors_test <- psych::nfactors(indices_selected)\n\n\nWarning in fac(r = r, nfactors = nfactors, n.obs = n.obs, rotate = rotate, : An\nultra-Heywood case was detected. Examine the results carefully\n\n\n\n\n\n\nCorrelated latent factors\n\n\n\nCode\nfit <- psych::fa(indices_selected, 3)\n\n\nLoading required namespace: GPArotation\n\n\nCode\npsych::fa.diagram(fit)\n\n\n\n\n\n\nBifactor model\n\n\n\nCode\nfit_bifac <- psych::omega(indices_selected, 3, plot = FALSE)\npsych::omega.diagram(fit_bifac)\n\n\n\n\n\n\n\nBayesian Factor Analysis\n\n\nCode\nmcmc <- indices_selected |> \n  mutate(across(.fns = ~ scale(.)[, 1])) |> \n  befa(verbose = FALSE) |> \n  post.column.switch() |> \n  post.sign.switch()\n\n\n\n\nCode\nhppm <- summary(mcmc, what = \"hppm\")\nhppm |> \n  pluck(\"alpha\", \"m1\") |> \n  as_tibble(rownames = \"alpha_term\") |> \n  separate(alpha_term, c(NA, \"game_index\"), sep = \":\") |> \n  mutate(game_index = reorder(game_index, dedic)) |> \n  ggplot(aes(game_index, dedic)) +\n  geom_tile(aes(fill = mean)) +\n  geom_text(aes(label = round(mean, 2)), color = \"white\") +\n  scico::scale_fill_scico(palette = \"bam\", midpoint = 0, direction = -1) +\n  coord_fixed() +\n  theme_minimal(base_size = 18) +\n  labs(x = \"Term\", y = \"Factor\", \n       title = str_c(\"Posterior Probability: \", round(hppm$hppm$prob, 2), \n                     \", with \", hppm$hppm$nfac, \" factors\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "posts/explore-struct-wm/index.html#confirmatory-factor-analysis",
    "href": "posts/explore-struct-wm/index.html#confirmatory-factor-analysis",
    "title": "Explore Cognitive Structure for Working Memory",
    "section": "Confirmatory Factor Analysis",
    "text": "Confirmatory Factor Analysis\n\nModel from EFA\nFirstly, the model from EFA is tested.\n\nSpatial-object association test (宇宙黑洞) included\n\nCorrelated latent factor model:\n\n\n\nCode\nfitted1 <- lavaan::cfa(\n  'Updating =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `宇宙黑洞` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted1, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted1, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 15 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        36\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        55\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                115.266     114.149\n  Degrees of freedom                                 41          41\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   1.010\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1347.235    1273.996\n  Degrees of freedom                                55          55\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.057\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.943       0.940\n  Tucker-Lewis Index (TLI)                       0.923       0.920\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.943\n  Robust Tucker-Lewis Index (TLI)                            0.923\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6185.158   -6185.158\n  Scaling correction factor                                  1.041\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -6127.525   -6127.525\n  Scaling correction factor                                  1.024\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               12442.315   12442.315\n  Bayesian (BIC)                             12591.737   12591.737\n  Sample-size adjusted Bayesian (BIC)        12477.480   12477.480\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.062       0.062\n  90 Percent confidence interval - lower         0.049       0.048\n  90 Percent confidence interval - upper         0.076       0.075\n  P-value RMSEA <= 0.05                          0.065       0.072\n                                                                  \n  Robust RMSEA                                               0.062\n  90 Percent confidence interval - lower                     0.049\n  90 Percent confidence interval - upper                     0.076\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.037       0.037\n\n\n\n\nCode\nfitted1_alt <- lavaan::cfa(\n  'Updating1 =~ `美术卡片` + `数字卡片`\n  Updating2 =~ `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `宇宙黑洞` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted1_alt, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted1_alt, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 20 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        39\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        55\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                 64.777      63.925\n  Degrees of freedom                                 38          38\n  P-value (Chi-square)                            0.004       0.005\n  Scaling correction factor                                   1.013\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1347.235    1273.996\n  Degrees of freedom                                55          55\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.057\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.979       0.979\n  Tucker-Lewis Index (TLI)                       0.970       0.969\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.980\n  Robust Tucker-Lewis Index (TLI)                            0.971\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6159.913   -6159.913\n  Scaling correction factor                                  1.035\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -6127.525   -6127.525\n  Scaling correction factor                                  1.024\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               12397.826   12397.826\n  Bayesian (BIC)                             12559.700   12559.700\n  Sample-size adjusted Bayesian (BIC)        12435.922   12435.922\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.039       0.038\n  90 Percent confidence interval - lower         0.022       0.021\n  90 Percent confidence interval - upper         0.055       0.054\n  P-value RMSEA <= 0.05                          0.872       0.886\n                                                                  \n  Robust RMSEA                                               0.038\n  90 Percent confidence interval - lower                     0.021\n  90 Percent confidence interval - upper                     0.054\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.031       0.031\n\n\n\n\nCode\nfitted1_alt2 <- lavaan::cfa(\n  'Updating =~ Updating1 + Updating2\n  Updating1 =~ `美术卡片` + `数字卡片`\n  Updating2 =~ `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `宇宙黑洞` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted1_alt2, what = \"std\", edge.color = \"black\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted1_alt2, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 32 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        38\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        55\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                 64.918      64.130\n  Degrees of freedom                                 39          39\n  P-value (Chi-square)                            0.006       0.007\n  Scaling correction factor                                   1.012\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1347.235    1273.996\n  Degrees of freedom                                55          55\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.057\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.980       0.979\n  Tucker-Lewis Index (TLI)                       0.972       0.971\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.980\n  Robust Tucker-Lewis Index (TLI)                            0.972\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6159.984   -6159.984\n  Scaling correction factor                                  1.037\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -6127.525   -6127.525\n  Scaling correction factor                                  1.024\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               12395.967   12395.967\n  Bayesian (BIC)                             12553.690   12553.690\n  Sample-size adjusted Bayesian (BIC)        12433.086   12433.086\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.038       0.037\n  90 Percent confidence interval - lower         0.020       0.020\n  90 Percent confidence interval - upper         0.053       0.053\n  P-value RMSEA <= 0.05                          0.897       0.908\n                                                                  \n  Robust RMSEA                                               0.037\n  90 Percent confidence interval - lower                     0.020\n  90 Percent confidence interval - upper                     0.053\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.031       0.031\n\n\n\nBifactor model:\n\n\n\nCode\nfitted1_bifac <- lavaan::cfa(\n  'Common =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片` + `幸运小球` + `密码箱` + `顺背数PRO` + `宇宙黑洞` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`\n  Updating =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `宇宙黑洞` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE, orthogonal = TRUE, \n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted1_bifac, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0,\n  bifactor = \"Common\"\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted1_bifac, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 45 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        44\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        55\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                 68.845      74.976\n  Degrees of freedom                                 33          33\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   0.918\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1347.235    1273.996\n  Degrees of freedom                                55          55\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.057\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.972       0.966\n  Tucker-Lewis Index (TLI)                       0.954       0.943\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.970\n  Robust Tucker-Lewis Index (TLI)                            0.950\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6161.947   -6161.947\n  Scaling correction factor                                  1.104\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -6127.525   -6127.525\n  Scaling correction factor                                  1.024\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               12411.894   12411.894\n  Bayesian (BIC)                             12594.521   12594.521\n  Sample-size adjusted Bayesian (BIC)        12454.873   12454.873\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.048       0.052\n  90 Percent confidence interval - lower         0.032       0.036\n  90 Percent confidence interval - upper         0.064       0.068\n  P-value RMSEA <= 0.05                          0.553       0.393\n                                                                  \n  Robust RMSEA                                               0.050\n  90 Percent confidence interval - lower                     0.035\n  90 Percent confidence interval - upper                     0.065\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.041       0.041\n\n\n\nHierarchical model:\n\n\n\nCode\nfitted1_higher <- lavaan::cfa(\n  'Common =~ Updating + VerbalSTM + SpatialSTM\n  Updating =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `宇宙黑洞` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted1_higher, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0,\n  bifactor = \"Common\"\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted1_higher, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 35 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        36\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        55\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                115.266     114.149\n  Degrees of freedom                                 41          41\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   1.010\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1347.235    1273.996\n  Degrees of freedom                                55          55\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.057\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.943       0.940\n  Tucker-Lewis Index (TLI)                       0.923       0.920\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.943\n  Robust Tucker-Lewis Index (TLI)                            0.923\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6185.158   -6185.158\n  Scaling correction factor                                  1.041\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -6127.525   -6127.525\n  Scaling correction factor                                  1.024\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               12442.315   12442.315\n  Bayesian (BIC)                             12591.737   12591.737\n  Sample-size adjusted Bayesian (BIC)        12477.480   12477.480\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.062       0.062\n  90 Percent confidence interval - lower         0.049       0.048\n  90 Percent confidence interval - upper         0.076       0.075\n  P-value RMSEA <= 0.05                          0.065       0.072\n                                                                  \n  Robust RMSEA                                               0.062\n  90 Percent confidence interval - lower                     0.049\n  90 Percent confidence interval - upper                     0.076\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.037       0.037\n\n\n\n\nSpatial-object association test (宇宙黑洞) excluded\n\nCorrelated latent factor model:\n\n\n\nCode\nfitted2 <- lavaan::cfa(\n  'Updating =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted2, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted2, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 15 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        33\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        50\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                 97.634      97.382\n  Degrees of freedom                                 32          32\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   1.003\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1251.825    1191.197\n  Degrees of freedom                                45          45\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.051\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.946       0.943\n  Tucker-Lewis Index (TLI)                       0.924       0.920\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.946\n  Robust Tucker-Lewis Index (TLI)                            0.923\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5595.957   -5595.957\n  Scaling correction factor                                  1.030\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5547.140   -5547.140\n  Scaling correction factor                                  1.017\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11257.914   11257.914\n  Bayesian (BIC)                             11394.884   11394.884\n  Sample-size adjusted Bayesian (BIC)        11290.148   11290.148\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.066       0.066\n  90 Percent confidence interval - lower         0.051       0.051\n  90 Percent confidence interval - upper         0.081       0.081\n  P-value RMSEA <= 0.05                          0.037       0.038\n                                                                  \n  Robust RMSEA                                               0.066\n  90 Percent confidence interval - lower                     0.051\n  90 Percent confidence interval - upper                     0.081\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.037       0.037\n\n\n\n\nCode\nfitted2_alt <- lavaan::cfa(\n  'Updating1 =~ `美术卡片` + `数字卡片`\n  Updating2 =~ `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted1_alt, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted1_alt, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 20 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        39\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        55\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                 64.777      63.925\n  Degrees of freedom                                 38          38\n  P-value (Chi-square)                            0.004       0.005\n  Scaling correction factor                                   1.013\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1347.235    1273.996\n  Degrees of freedom                                55          55\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.057\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.979       0.979\n  Tucker-Lewis Index (TLI)                       0.970       0.969\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.980\n  Robust Tucker-Lewis Index (TLI)                            0.971\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6159.913   -6159.913\n  Scaling correction factor                                  1.035\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -6127.525   -6127.525\n  Scaling correction factor                                  1.024\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               12397.826   12397.826\n  Bayesian (BIC)                             12559.700   12559.700\n  Sample-size adjusted Bayesian (BIC)        12435.922   12435.922\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.039       0.038\n  90 Percent confidence interval - lower         0.022       0.021\n  90 Percent confidence interval - upper         0.055       0.054\n  P-value RMSEA <= 0.05                          0.872       0.886\n                                                                  \n  Robust RMSEA                                               0.038\n  90 Percent confidence interval - lower                     0.021\n  90 Percent confidence interval - upper                     0.054\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.031       0.031\n\n\n\n\nCode\nfitted2_alt2 <- lavaan::cfa(\n  'Updating =~ Updating1 + Updating2\n  Updating1 =~ `美术卡片` + `数字卡片`\n  Updating2 =~ `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted2_alt2, what = \"std\", edge.color = \"black\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted2_alt2, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 33 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        35\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        50\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                 47.237      46.994\n  Degrees of freedom                                 30          30\n  P-value (Chi-square)                            0.024       0.025\n  Scaling correction factor                                   1.005\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1251.825    1191.197\n  Degrees of freedom                                45          45\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.051\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.986       0.985\n  Tucker-Lewis Index (TLI)                       0.979       0.978\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.986\n  Robust Tucker-Lewis Index (TLI)                            0.979\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5570.758   -5570.758\n  Scaling correction factor                                  1.027\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5547.140   -5547.140\n  Scaling correction factor                                  1.017\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11211.517   11211.517\n  Bayesian (BIC)                             11356.788   11356.788\n  Sample-size adjusted Bayesian (BIC)        11245.705   11245.705\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.035       0.035\n  90 Percent confidence interval - lower         0.013       0.013\n  90 Percent confidence interval - upper         0.053       0.053\n  P-value RMSEA <= 0.05                          0.906       0.911\n                                                                  \n  Robust RMSEA                                               0.035\n  90 Percent confidence interval - lower                     0.013\n  90 Percent confidence interval - upper                     0.053\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.029       0.029\n\n\n\nBifactor model:\n\n\n\nCode\nfitted2_bifac <- lavaan::cfa(\n  'Common =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片` + `幸运小球` + `密码箱` + `顺背数PRO` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`\n  Updating =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE, orthogonal = TRUE, \n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted2_bifac, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0,\n  bifactor = \"Common\"\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted2_bifac, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 45 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        40\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        50\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                 53.281      61.852\n  Degrees of freedom                                 25          25\n  P-value (Chi-square)                            0.001       0.000\n  Scaling correction factor                                   0.861\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1251.825    1191.197\n  Degrees of freedom                                45          45\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.051\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.977       0.968\n  Tucker-Lewis Index (TLI)                       0.958       0.942\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.974\n  Robust Tucker-Lewis Index (TLI)                            0.953\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5573.780   -5573.780\n  Scaling correction factor                                  1.114\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5547.140   -5547.140\n  Scaling correction factor                                  1.017\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11227.561   11227.561\n  Bayesian (BIC)                             11393.585   11393.585\n  Sample-size adjusted Bayesian (BIC)        11266.633   11266.633\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.049       0.056\n  90 Percent confidence interval - lower         0.031       0.037\n  90 Percent confidence interval - upper         0.067       0.075\n  P-value RMSEA <= 0.05                          0.504       0.276\n                                                                  \n  Robust RMSEA                                               0.052\n  90 Percent confidence interval - lower                     0.036\n  90 Percent confidence interval - upper                     0.069\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.040       0.040\n\n\n\nHierarchical Model:\n\n\n\nCode\nfitted2_higher <- lavaan::cfa(\n  'Common =~ Updating + VerbalSTM + SpatialSTM\n  Updating =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted2_higher, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0,\n  bifactor = \"Common\"\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted2_higher, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 34 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        33\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        50\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                 97.634      97.382\n  Degrees of freedom                                 32          32\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   1.003\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1251.825    1191.197\n  Degrees of freedom                                45          45\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.051\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.946       0.943\n  Tucker-Lewis Index (TLI)                       0.924       0.920\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.946\n  Robust Tucker-Lewis Index (TLI)                            0.923\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5595.957   -5595.957\n  Scaling correction factor                                  1.030\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5547.140   -5547.140\n  Scaling correction factor                                  1.017\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11257.914   11257.914\n  Bayesian (BIC)                             11394.884   11394.884\n  Sample-size adjusted Bayesian (BIC)        11290.148   11290.148\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.066       0.066\n  90 Percent confidence interval - lower         0.051       0.051\n  90 Percent confidence interval - upper         0.081       0.081\n  P-value RMSEA <= 0.05                          0.037       0.038\n                                                                  \n  Robust RMSEA                                               0.066\n  90 Percent confidence interval - lower                     0.051\n  90 Percent confidence interval - upper                     0.081\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.037       0.037\n\n\n\n\n\nTask Structure Model\n\nThe covariance estimate for latent variables is not correct. The estimated correlation between “Simple” and “Complex” is larger than 1.\n\n\n\nCode\nfitted3 <- lavaan::cfa(\n  'Complex =~ `打靶场` + `蝴蝶照相机` + `幸运小球`\n  Nback =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  Simple =~ `密码箱` + `顺背数PRO` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE, \n  estimator = \"MLR\", missing = \"ml\"\n)\n\n\nWarning in lav_object_post_check(object): lavaan WARNING: covariance matrix of latent variables\n                is not positive definite;\n                use lavInspect(fit, \"cov.lv\") to investigate.\n\n\nCode\nsemPlot::semPaths(\n  fitted3, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted3, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 17 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        33\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        50\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                171.820     173.236\n  Degrees of freedom                                 32          32\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   0.992\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1251.825    1191.197\n  Degrees of freedom                                45          45\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.051\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.884       0.877\n  Tucker-Lewis Index (TLI)                       0.837       0.827\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.884\n  Robust Tucker-Lewis Index (TLI)                            0.836\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5633.050   -5633.050\n  Scaling correction factor                                  1.041\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5547.140   -5547.140\n  Scaling correction factor                                  1.017\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11332.100   11332.100\n  Bayesian (BIC)                             11469.070   11469.070\n  Sample-size adjusted Bayesian (BIC)        11364.335   11364.335\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.097       0.097\n  90 Percent confidence interval - lower         0.083       0.083\n  90 Percent confidence interval - upper         0.111       0.111\n  P-value RMSEA <= 0.05                          0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.097\n  90 Percent confidence interval - lower                     0.083\n  90 Percent confidence interval - upper                     0.111\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.054       0.054\n\n\n\n\nInput Material Model\n\nNot fitted well.\n\n\n\nCode\nfitted4 <- lavaan::cfa(\n  'Verbal =~ 数字卡片 + 文字卡片 + 幸运小球 + 密码箱 + 顺背数PRO\n  Spatial =~ 格子卡片 + 打靶场 + 蝴蝶照相机 + 位置记忆PRO\n  Object =~ 美术卡片 + 宇宙黑洞',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted4, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted4, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 19 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        36\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        55\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                286.789     289.021\n  Degrees of freedom                                 41          41\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   0.992\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1347.235    1273.996\n  Degrees of freedom                                55          55\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.057\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.810       0.797\n  Tucker-Lewis Index (TLI)                       0.745       0.727\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.809\n  Robust Tucker-Lewis Index (TLI)                            0.744\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6270.919   -6270.919\n  Scaling correction factor                                  1.061\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -6127.525   -6127.525\n  Scaling correction factor                                  1.024\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               12613.838   12613.838\n  Bayesian (BIC)                             12763.259   12763.259\n  Sample-size adjusted Bayesian (BIC)        12649.003   12649.003\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.113       0.114\n  90 Percent confidence interval - lower         0.101       0.101\n  90 Percent confidence interval - upper         0.126       0.126\n  P-value RMSEA <= 0.05                          0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.113\n  90 Percent confidence interval - lower                     0.101\n  90 Percent confidence interval - upper                     0.126\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.066       0.066"
  },
  {
    "objectID": "posts/explore-struct-wm/index.html#correlation-matrix-1",
    "href": "posts/explore-struct-wm/index.html#correlation-matrix-1",
    "title": "Explore Cognitive Structure for Working Memory",
    "section": "Correlation Matrix",
    "text": "Correlation Matrix\n\n\nCode\ncorrelate(indices_selected, quiet = TRUE) |> \n  rearrange(method = \"HC\") |>\n  stretch() |> \n  mutate(across(c(x, y), as_factor)) |>\n  ggplot(aes(x, y)) +\n  geom_tile(aes(fill = r)) +\n  scico::scale_fill_scico(palette = \"bam\", midpoint = 0, direction = -1) +\n  coord_fixed() +\n  theme_minimal(base_size = 18) +\n  labs(x = \"\", y = \"\", fill = \"Pearson's\", color = \"\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "posts/explore-struct-wm/index.html#multidimensional-scaling-1",
    "href": "posts/explore-struct-wm/index.html#multidimensional-scaling-1",
    "title": "Explore Cognitive Structure for Working Memory",
    "section": "Multidimensional Scaling",
    "text": "Multidimensional Scaling\n\n\nCode\nmds <- indices_selected |> \n  cor(use = \"pairwise\") |> \n  smacof::sim2diss(to.dist = TRUE) |> \n  smacof::mds(ndim = 2, type = \"mspline\")\nplot(mds)"
  },
  {
    "objectID": "posts/explore-struct-wm/index.html#exploratory-factor-analysis-1",
    "href": "posts/explore-struct-wm/index.html#exploratory-factor-analysis-1",
    "title": "Explore Cognitive Structure for Working Memory",
    "section": "Exploratory Factor Analysis",
    "text": "Exploratory Factor Analysis\n\nTraditional\n\n\nCode\nnfactors_test <- psych::nfactors(indices_selected)\n\n\n\n\n\n\nCorrelated latent factors\n\n\n\nCode\nfit <- psych::fa(indices_selected, 3)\npsych::fa.diagram(fit)\n\n\n\n\n\n\nBifactor model\n\n\n\nCode\nfit_bifac <- psych::omega(indices_selected, 3, plot = FALSE)\npsych::omega.diagram(fit_bifac)\n\n\n\n\n\n\n\nBayesian Factor Analysis\n\n\nCode\nmcmc <- indices_selected |> \n  mutate(across(.fns = ~ scale(.)[, 1])) |> \n  befa(verbose = FALSE) |> \n  post.column.switch() |> \n  post.sign.switch()\n\n\n\n\nCode\nhppm <- summary(mcmc, what = \"hppm\")\nhppm |> \n  pluck(\"alpha\", \"m1\") |> \n  as_tibble(rownames = \"alpha_term\") |> \n  separate(alpha_term, c(NA, \"game_index\"), sep = \":\") |> \n  mutate(game_index = reorder(game_index, dedic)) |> \n  ggplot(aes(game_index, dedic)) +\n  geom_tile(aes(fill = mean)) +\n  geom_text(aes(label = round(mean, 2)), color = \"white\") +\n  scico::scale_fill_scico(palette = \"bam\", midpoint = 0, direction = -1) +\n  coord_fixed() +\n  theme_minimal(base_size = 18) +\n  labs(x = \"Term\", y = \"Factor\", \n       title = str_c(\"Posterior Probability: \", round(hppm$hppm$prob, 2), \n                     \", with \", hppm$hppm$nfac, \" factors\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "posts/explore-struct-wm/index.html#confirmatory-factor-analysis-1",
    "href": "posts/explore-struct-wm/index.html#confirmatory-factor-analysis-1",
    "title": "Explore Cognitive Structure for Working Memory",
    "section": "Confirmatory Factor Analysis",
    "text": "Confirmatory Factor Analysis\n\nModel from EFA\nFirstly, the model from EFA is tested.\n\nSpatial-object association test (宇宙黑洞) included\n\nCorrelated latent factor model:\n\n\n\nCode\nfitted1 <- lavaan::cfa(\n  'Updating =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `宇宙黑洞` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted1, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted1, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 16 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        36\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        47\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                117.168     115.187\n  Degrees of freedom                                 41          41\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   1.017\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1397.256    1319.621\n  Degrees of freedom                                55          55\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.059\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.943       0.941\n  Tucker-Lewis Index (TLI)                       0.924       0.921\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.944\n  Robust Tucker-Lewis Index (TLI)                            0.924\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6192.315   -6192.315\n  Scaling correction factor                                  1.024\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -6133.731   -6133.731\n  Scaling correction factor                                  1.020\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               12456.630   12456.630\n  Bayesian (BIC)                             12606.051   12606.051\n  Sample-size adjusted Bayesian (BIC)        12491.794   12491.794\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.063       0.062\n  90 Percent confidence interval - lower         0.050       0.049\n  90 Percent confidence interval - upper         0.076       0.076\n  P-value RMSEA <= 0.05                          0.053       0.064\n                                                                  \n  Robust RMSEA                                               0.063\n  90 Percent confidence interval - lower                     0.049\n  90 Percent confidence interval - upper                     0.076\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.036       0.036\n\n\n\n\nCode\nfitted1_alt <- lavaan::cfa(\n  'Updating1 =~ `美术卡片` + `数字卡片`\n  Updating2 =~ `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `宇宙黑洞` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted1_alt, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted1_alt, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 20 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        39\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        47\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                 65.040      63.632\n  Degrees of freedom                                 38          38\n  P-value (Chi-square)                            0.004       0.006\n  Scaling correction factor                                   1.022\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1397.256    1319.621\n  Degrees of freedom                                55          55\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.059\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.980       0.980\n  Tucker-Lewis Index (TLI)                       0.971       0.971\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.980\n  Robust Tucker-Lewis Index (TLI)                            0.972\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6166.251   -6166.251\n  Scaling correction factor                                  1.019\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -6133.731   -6133.731\n  Scaling correction factor                                  1.020\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               12410.501   12410.501\n  Bayesian (BIC)                             12572.375   12572.375\n  Sample-size adjusted Bayesian (BIC)        12448.597   12448.597\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.039       0.038\n  90 Percent confidence interval - lower         0.022       0.021\n  90 Percent confidence interval - upper         0.055       0.054\n  P-value RMSEA <= 0.05                          0.868       0.891\n                                                                  \n  Robust RMSEA                                               0.038\n  90 Percent confidence interval - lower                     0.021\n  90 Percent confidence interval - upper                     0.054\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.030       0.030\n\n\n\n\nCode\nfitted1_alt2 <- lavaan::cfa(\n  'Updating =~ Updating1 + Updating2\n  Updating1 =~ `美术卡片` + `数字卡片`\n  Updating2 =~ `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `宇宙黑洞` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted1_alt2, what = \"std\", edge.color = \"black\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted1_alt2, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 33 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        38\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        47\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                 65.067      63.695\n  Degrees of freedom                                 39          39\n  P-value (Chi-square)                            0.005       0.008\n  Scaling correction factor                                   1.022\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1397.256    1319.621\n  Degrees of freedom                                55          55\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.059\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.981       0.980\n  Tucker-Lewis Index (TLI)                       0.973       0.972\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.981\n  Robust Tucker-Lewis Index (TLI)                            0.973\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6166.264   -6166.264\n  Scaling correction factor                                  1.019\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -6133.731   -6133.731\n  Scaling correction factor                                  1.020\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               12408.528   12408.528\n  Bayesian (BIC)                             12566.251   12566.251\n  Sample-size adjusted Bayesian (BIC)        12445.647   12445.647\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.038       0.037\n  90 Percent confidence interval - lower         0.021       0.019\n  90 Percent confidence interval - upper         0.053       0.052\n  P-value RMSEA <= 0.05                          0.895       0.915\n                                                                  \n  Robust RMSEA                                               0.037\n  90 Percent confidence interval - lower                     0.019\n  90 Percent confidence interval - upper                     0.053\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.030       0.030\n\n\n\nBifactor model:\n\n\n\nCode\nfitted1_bifac <- lavaan::cfa(\n  'Common =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片` + `幸运小球` + `密码箱` + `顺背数PRO` + `宇宙黑洞` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`\n  Updating =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `宇宙黑洞` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE, orthogonal = TRUE, \n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted1_bifac, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0,\n  bifactor = \"Common\"\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted1_bifac, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 47 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        44\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        47\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                 72.027      77.580\n  Degrees of freedom                                 33          33\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   0.928\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1397.256    1319.621\n  Degrees of freedom                                55          55\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.059\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.971       0.965\n  Tucker-Lewis Index (TLI)                       0.952       0.941\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.969\n  Robust Tucker-Lewis Index (TLI)                            0.948\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6169.744   -6169.744\n  Scaling correction factor                                  1.089\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -6133.731   -6133.731\n  Scaling correction factor                                  1.020\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               12427.488   12427.488\n  Bayesian (BIC)                             12610.115   12610.115\n  Sample-size adjusted Bayesian (BIC)        12470.467   12470.467\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.050       0.054\n  90 Percent confidence interval - lower         0.034       0.038\n  90 Percent confidence interval - upper         0.066       0.070\n  P-value RMSEA <= 0.05                          0.467       0.332\n                                                                  \n  Robust RMSEA                                               0.052\n  90 Percent confidence interval - lower                     0.037\n  90 Percent confidence interval - upper                     0.067\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.042       0.042\n\n\n\nHierarchical model:\n\n\n\nCode\nfitted1_higher <- lavaan::cfa(\n  'Common =~ Updating + VerbalSTM + SpatialSTM\n  Updating =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `宇宙黑洞` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted1_higher, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0,\n  bifactor = \"Common\"\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted1_higher, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 34 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        36\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        47\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                117.168     115.187\n  Degrees of freedom                                 41          41\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   1.017\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1397.256    1319.621\n  Degrees of freedom                                55          55\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.059\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.943       0.941\n  Tucker-Lewis Index (TLI)                       0.924       0.921\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.944\n  Robust Tucker-Lewis Index (TLI)                            0.924\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6192.315   -6192.315\n  Scaling correction factor                                  1.024\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -6133.731   -6133.731\n  Scaling correction factor                                  1.020\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               12456.630   12456.630\n  Bayesian (BIC)                             12606.051   12606.051\n  Sample-size adjusted Bayesian (BIC)        12491.794   12491.794\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.063       0.062\n  90 Percent confidence interval - lower         0.050       0.049\n  90 Percent confidence interval - upper         0.076       0.076\n  P-value RMSEA <= 0.05                          0.053       0.064\n                                                                  \n  Robust RMSEA                                               0.063\n  90 Percent confidence interval - lower                     0.049\n  90 Percent confidence interval - upper                     0.076\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.036       0.036\n\n\n\n\nSpatial-object association test (宇宙黑洞) excluded\n\nCorrelated latent factor model:\n\n\n\nCode\nfitted2 <- lavaan::cfa(\n  'Updating =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted2, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted2, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 17 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        33\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        43\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                 99.969      98.539\n  Degrees of freedom                                 32          32\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   1.015\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1301.556    1234.105\n  Degrees of freedom                                45          45\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.055\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.946       0.944\n  Tucker-Lewis Index (TLI)                       0.924       0.921\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.946\n  Robust Tucker-Lewis Index (TLI)                            0.924\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5603.476   -5603.476\n  Scaling correction factor                                  1.013\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5553.491   -5553.491\n  Scaling correction factor                                  1.014\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11272.952   11272.952\n  Bayesian (BIC)                             11409.921   11409.921\n  Sample-size adjusted Bayesian (BIC)        11305.186   11305.186\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.067       0.067\n  90 Percent confidence interval - lower         0.053       0.052\n  90 Percent confidence interval - upper         0.082       0.082\n  P-value RMSEA <= 0.05                          0.027       0.032\n                                                                  \n  Robust RMSEA                                               0.067\n  90 Percent confidence interval - lower                     0.052\n  90 Percent confidence interval - upper                     0.082\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.036       0.036\n\n\n\n\nCode\nfitted2_alt <- lavaan::cfa(\n  'Updating1 =~ `美术卡片` + `数字卡片`\n  Updating2 =~ `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted1_alt, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted1_alt, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 20 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        39\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        47\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                 65.040      63.632\n  Degrees of freedom                                 38          38\n  P-value (Chi-square)                            0.004       0.006\n  Scaling correction factor                                   1.022\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1397.256    1319.621\n  Degrees of freedom                                55          55\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.059\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.980       0.980\n  Tucker-Lewis Index (TLI)                       0.971       0.971\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.980\n  Robust Tucker-Lewis Index (TLI)                            0.972\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6166.251   -6166.251\n  Scaling correction factor                                  1.019\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -6133.731   -6133.731\n  Scaling correction factor                                  1.020\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               12410.501   12410.501\n  Bayesian (BIC)                             12572.375   12572.375\n  Sample-size adjusted Bayesian (BIC)        12448.597   12448.597\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.039       0.038\n  90 Percent confidence interval - lower         0.022       0.021\n  90 Percent confidence interval - upper         0.055       0.054\n  P-value RMSEA <= 0.05                          0.868       0.891\n                                                                  \n  Robust RMSEA                                               0.038\n  90 Percent confidence interval - lower                     0.021\n  90 Percent confidence interval - upper                     0.054\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.030       0.030\n\n\n\n\nCode\nfitted2_alt2 <- lavaan::cfa(\n  'Updating =~ Updating1 + Updating2\n  Updating1 =~ `美术卡片` + `数字卡片`\n  Updating2 =~ `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted2_alt2, what = \"std\", edge.color = \"black\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted2_alt2, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 34 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        35\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        43\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                 47.868      46.942\n  Degrees of freedom                                 30          30\n  P-value (Chi-square)                            0.020       0.025\n  Scaling correction factor                                   1.020\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1301.556    1234.105\n  Degrees of freedom                                45          45\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.055\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.986       0.986\n  Tucker-Lewis Index (TLI)                       0.979       0.979\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.986\n  Robust Tucker-Lewis Index (TLI)                            0.979\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5577.425   -5577.425\n  Scaling correction factor                                  1.009\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5553.491   -5553.491\n  Scaling correction factor                                  1.014\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11224.851   11224.851\n  Bayesian (BIC)                             11370.122   11370.122\n  Sample-size adjusted Bayesian (BIC)        11259.039   11259.039\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.036       0.035\n  90 Percent confidence interval - lower         0.014       0.013\n  90 Percent confidence interval - upper         0.054       0.053\n  P-value RMSEA <= 0.05                          0.897       0.913\n                                                                  \n  Robust RMSEA                                               0.035\n  90 Percent confidence interval - lower                     0.013\n  90 Percent confidence interval - upper                     0.054\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.029       0.029\n\n\n\nBifactor model:\n\n\n\nCode\nfitted2_bifac <- lavaan::cfa(\n  'Common =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片` + `幸运小球` + `密码箱` + `顺背数PRO` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`\n  Updating =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE, orthogonal = TRUE, \n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted2_bifac, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0,\n  bifactor = \"Common\"\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted2_bifac, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 49 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        40\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        43\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                 57.251      80.402\n  Degrees of freedom                                 25          25\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   0.712\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1301.556    1234.105\n  Degrees of freedom                                45          45\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.055\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.974       0.953\n  Tucker-Lewis Index (TLI)                       0.954       0.916\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.969\n  Robust Tucker-Lewis Index (TLI)                            0.943\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5582.117   -5582.117\n  Scaling correction factor                                  1.202\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5553.491   -5553.491\n  Scaling correction factor                                  1.014\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11244.234   11244.234\n  Bayesian (BIC)                             11410.258   11410.258\n  Sample-size adjusted Bayesian (BIC)        11283.306   11283.306\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.052       0.069\n  90 Percent confidence interval - lower         0.035       0.049\n  90 Percent confidence interval - upper         0.070       0.089\n  P-value RMSEA <= 0.05                          0.385       0.056\n                                                                  \n  Robust RMSEA                                               0.058\n  90 Percent confidence interval - lower                     0.044\n  90 Percent confidence interval - upper                     0.072\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.042       0.042\n\n\n\nHierarchical Model:\n\n\n\nCode\nfitted2_higher <- lavaan::cfa(\n  'Common =~ Updating + VerbalSTM + SpatialSTM\n  Updating =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted2_higher, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0,\n  bifactor = \"Common\"\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted2_higher, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 33 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        33\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        43\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                 99.969      98.539\n  Degrees of freedom                                 32          32\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   1.015\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1301.556    1234.105\n  Degrees of freedom                                45          45\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.055\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.946       0.944\n  Tucker-Lewis Index (TLI)                       0.924       0.921\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.946\n  Robust Tucker-Lewis Index (TLI)                            0.924\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5603.476   -5603.476\n  Scaling correction factor                                  1.013\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5553.491   -5553.491\n  Scaling correction factor                                  1.014\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11272.952   11272.952\n  Bayesian (BIC)                             11409.921   11409.921\n  Sample-size adjusted Bayesian (BIC)        11305.186   11305.186\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.067       0.067\n  90 Percent confidence interval - lower         0.053       0.052\n  90 Percent confidence interval - upper         0.082       0.082\n  P-value RMSEA <= 0.05                          0.027       0.032\n                                                                  \n  Robust RMSEA                                               0.067\n  90 Percent confidence interval - lower                     0.052\n  90 Percent confidence interval - upper                     0.082\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.036       0.036\n\n\n\n\n\nTask Structure Model\n\nThe covariance estimate for latent variables is not correct. The estimated correlation between “Simple” and “Complex” is larger than 1.\n\n\n\nCode\nfitted3 <- lavaan::cfa(\n  'Complex =~ `打靶场` + `蝴蝶照相机` + `幸运小球`\n  Nback =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  Simple =~ `密码箱` + `顺背数PRO` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE, \n  estimator = \"MLR\", missing = \"ml\"\n)\n\n\nWarning in lav_object_post_check(object): lavaan WARNING: covariance matrix of latent variables\n                is not positive definite;\n                use lavInspect(fit, \"cov.lv\") to investigate.\n\n\nCode\nsemPlot::semPaths(\n  fitted3, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted3, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 17 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        33\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        43\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                173.487     172.308\n  Degrees of freedom                                 32          32\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   1.007\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1301.556    1234.105\n  Degrees of freedom                                45          45\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.055\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.887       0.882\n  Tucker-Lewis Index (TLI)                       0.842       0.834\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.887\n  Robust Tucker-Lewis Index (TLI)                            0.842\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5640.235   -5640.235\n  Scaling correction factor                                  1.020\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5553.491   -5553.491\n  Scaling correction factor                                  1.014\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11346.470   11346.470\n  Bayesian (BIC)                             11483.440   11483.440\n  Sample-size adjusted Bayesian (BIC)        11378.704   11378.704\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.097       0.097\n  90 Percent confidence interval - lower         0.083       0.083\n  90 Percent confidence interval - upper         0.112       0.111\n  P-value RMSEA <= 0.05                          0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.097\n  90 Percent confidence interval - lower                     0.083\n  90 Percent confidence interval - upper                     0.111\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.053       0.053\n\n\n\n\nInput Material Model\n\nNot fitted well.\n\n\n\nCode\nfitted4 <- lavaan::cfa(\n  'Verbal =~ 数字卡片 + 文字卡片 + 幸运小球 + 密码箱 + 顺背数PRO\n  Spatial =~ 格子卡片 + 打靶场 + 蝴蝶照相机 + 位置记忆PRO\n  Object =~ 美术卡片 + 宇宙黑洞',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\n\n\nWarning in lav_object_post_check(object): lavaan WARNING: covariance matrix of latent variables\n                is not positive definite;\n                use lavInspect(fit, \"cov.lv\") to investigate.\n\n\nCode\nsemPlot::semPaths(\n  fitted4, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted4, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 20 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        36\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        47\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                296.008     298.012\n  Degrees of freedom                                 41          41\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   0.993\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1397.256    1319.621\n  Degrees of freedom                                55          55\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.059\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.810       0.797\n  Tucker-Lewis Index (TLI)                       0.745       0.727\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.809\n  Robust Tucker-Lewis Index (TLI)                            0.744\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6281.735   -6281.735\n  Scaling correction factor                                  1.051\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -6133.731   -6133.731\n  Scaling correction factor                                  1.020\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               12635.470   12635.470\n  Bayesian (BIC)                             12784.891   12784.891\n  Sample-size adjusted Bayesian (BIC)        12670.634   12670.634\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.115       0.116\n  90 Percent confidence interval - lower         0.103       0.103\n  90 Percent confidence interval - upper         0.128       0.128\n  P-value RMSEA <= 0.05                          0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.115\n  90 Percent confidence interval - lower                     0.103\n  90 Percent confidence interval - upper                     0.128\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.068       0.068"
  },
  {
    "objectID": "posts/explore-struct-wm/index.html#correlation-matrix-2",
    "href": "posts/explore-struct-wm/index.html#correlation-matrix-2",
    "title": "Explore Cognitive Structure for Working Memory",
    "section": "Correlation Matrix",
    "text": "Correlation Matrix\n\n\nCode\ncorrelate(indices_selected, quiet = TRUE) |> \n  rearrange(method = \"HC\") |>\n  stretch() |> \n  mutate(across(c(x, y), as_factor)) |>\n  ggplot(aes(x, y)) +\n  geom_tile(aes(fill = r)) +\n  scico::scale_fill_scico(palette = \"bam\", midpoint = 0, direction = -1) +\n  coord_fixed() +\n  theme_minimal(base_size = 18) +\n  labs(x = \"\", y = \"\", fill = \"Pearson's\", color = \"\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "posts/explore-struct-wm/index.html#multidimensional-scaling-2",
    "href": "posts/explore-struct-wm/index.html#multidimensional-scaling-2",
    "title": "Explore Cognitive Structure for Working Memory",
    "section": "Multidimensional Scaling",
    "text": "Multidimensional Scaling\n\n\nCode\nmds <- indices_selected |> \n  cor(use = \"pairwise\") |> \n  smacof::sim2diss(to.dist = TRUE) |> \n  smacof::mds(ndim = 2, type = \"mspline\")\nplot(mds)"
  },
  {
    "objectID": "posts/explore-struct-wm/index.html#exploratory-factor-analysis-2",
    "href": "posts/explore-struct-wm/index.html#exploratory-factor-analysis-2",
    "title": "Explore Cognitive Structure for Working Memory",
    "section": "Exploratory Factor Analysis",
    "text": "Exploratory Factor Analysis\n\nTraditional\n\n\nCode\nnfactors_test <- psych::nfactors(indices_selected)\n\n\n\n\n\n\nCorrelated latent factors\n\n\n\nCode\nfit <- psych::fa(indices_selected, 3)\npsych::fa.diagram(fit)\n\n\n\n\n\n\nBifactor model\n\n\n\nCode\nfit_bifac <- psych::omega(indices_selected, 3, plot = FALSE)\npsych::omega.diagram(fit_bifac)\n\n\n\n\n\n\n\nBayesian Factor Analysis\n\n\nCode\nmcmc <- indices_selected |> \n  mutate(across(.fns = ~ scale(.)[, 1])) |> \n  befa(verbose = FALSE) |> \n  post.column.switch() |> \n  post.sign.switch()\n\n\n\n\nCode\nhppm <- summary(mcmc, what = \"hppm\")\nhppm |> \n  pluck(\"alpha\", \"m1\") |> \n  as_tibble(rownames = \"alpha_term\") |> \n  separate(alpha_term, c(NA, \"game_index\"), sep = \":\") |> \n  mutate(game_index = reorder(game_index, dedic)) |> \n  ggplot(aes(game_index, dedic)) +\n  geom_tile(aes(fill = mean)) +\n  geom_text(aes(label = round(mean, 2)), color = \"white\") +\n  scico::scale_fill_scico(palette = \"bam\", midpoint = 0, direction = -1) +\n  coord_fixed() +\n  theme_minimal(base_size = 18) +\n  labs(x = \"Term\", y = \"Factor\", \n       title = str_c(\"Posterior Probability: \", round(hppm$hppm$prob, 2), \n                     \", with \", hppm$hppm$nfac, \" factors\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "posts/explore-struct-wm/index.html#confirmatory-factor-analysis-2",
    "href": "posts/explore-struct-wm/index.html#confirmatory-factor-analysis-2",
    "title": "Explore Cognitive Structure for Working Memory",
    "section": "Confirmatory Factor Analysis",
    "text": "Confirmatory Factor Analysis\n\nModel from EFA\nFirstly, the model from EFA is tested.\n\nSpatial-object association test (宇宙黑洞) included\n\nCorrelated latent factor model:\n\n\n\nCode\nfitted1 <- lavaan::cfa(\n  'Updating =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `宇宙黑洞` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted1, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted1, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 18 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        36\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        45\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                119.456     116.993\n  Degrees of freedom                                 41          41\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   1.021\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1417.507    1333.075\n  Degrees of freedom                                55          55\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.063\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.942       0.941\n  Tucker-Lewis Index (TLI)                       0.923       0.920\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.943\n  Robust Tucker-Lewis Index (TLI)                            0.923\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6197.522   -6197.522\n  Scaling correction factor                                  1.025\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -6137.794   -6137.794\n  Scaling correction factor                                  1.023\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               12467.044   12467.044\n  Bayesian (BIC)                             12616.466   12616.466\n  Sample-size adjusted Bayesian (BIC)        12502.209   12502.209\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.064       0.063\n  90 Percent confidence interval - lower         0.051       0.050\n  90 Percent confidence interval - upper         0.077       0.076\n  P-value RMSEA <= 0.05                          0.042       0.053\n                                                                  \n  Robust RMSEA                                               0.064\n  90 Percent confidence interval - lower                     0.050\n  90 Percent confidence interval - upper                     0.077\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.037       0.037\n\n\n\n\nCode\nfitted1_alt <- lavaan::cfa(\n  'Updating1 =~ `美术卡片` + `数字卡片`\n  Updating2 =~ `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `宇宙黑洞` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted1_alt, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted1_alt, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 21 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        39\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        45\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                 67.870      66.281\n  Degrees of freedom                                 38          38\n  P-value (Chi-square)                            0.002       0.003\n  Scaling correction factor                                   1.024\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1417.507    1333.075\n  Degrees of freedom                                55          55\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.063\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.978       0.978\n  Tucker-Lewis Index (TLI)                       0.968       0.968\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.979\n  Robust Tucker-Lewis Index (TLI)                            0.969\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6171.729   -6171.729\n  Scaling correction factor                                  1.021\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -6137.794   -6137.794\n  Scaling correction factor                                  1.023\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               12421.459   12421.459\n  Bayesian (BIC)                             12583.332   12583.332\n  Sample-size adjusted Bayesian (BIC)        12459.554   12459.554\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.041       0.040\n  90 Percent confidence interval - lower         0.025       0.023\n  90 Percent confidence interval - upper         0.056       0.055\n  P-value RMSEA <= 0.05                          0.820       0.851\n                                                                  \n  Robust RMSEA                                               0.040\n  90 Percent confidence interval - lower                     0.023\n  90 Percent confidence interval - upper                     0.056\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.031       0.031\n\n\n\n\nCode\nfitted1_alt2 <- lavaan::cfa(\n  'Updating =~ Updating1 + Updating2\n  Updating1 =~ `美术卡片` + `数字卡片`\n  Updating2 =~ `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `宇宙黑洞` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted1_alt2, what = \"std\", edge.color = \"black\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted1_alt2, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 33 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        38\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        45\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                 68.030      66.505\n  Degrees of freedom                                 39          39\n  P-value (Chi-square)                            0.003       0.004\n  Scaling correction factor                                   1.023\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1417.507    1333.075\n  Degrees of freedom                                55          55\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.063\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.979       0.978\n  Tucker-Lewis Index (TLI)                       0.970       0.970\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.979\n  Robust Tucker-Lewis Index (TLI)                            0.971\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6171.809   -6171.809\n  Scaling correction factor                                  1.022\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -6137.794   -6137.794\n  Scaling correction factor                                  1.023\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               12419.618   12419.618\n  Bayesian (BIC)                             12577.341   12577.341\n  Sample-size adjusted Bayesian (BIC)        12456.737   12456.737\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.040       0.039\n  90 Percent confidence interval - lower         0.023       0.022\n  90 Percent confidence interval - upper         0.055       0.054\n  P-value RMSEA <= 0.05                          0.851       0.878\n                                                                  \n  Robust RMSEA                                               0.039\n  90 Percent confidence interval - lower                     0.022\n  90 Percent confidence interval - upper                     0.055\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.031       0.031\n\n\n\nBifactor model:\n\n\n\nCode\nfitted1_bifac <- lavaan::cfa(\n  'Common =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片` + `幸运小球` + `密码箱` + `顺背数PRO` + `宇宙黑洞` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`\n  Updating =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `宇宙黑洞` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE, orthogonal = TRUE, \n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted1_bifac, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0,\n  bifactor = \"Common\"\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted1_bifac, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 42 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        44\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        45\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                 76.215      86.440\n  Degrees of freedom                                 33          33\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   0.882\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1417.507    1333.075\n  Degrees of freedom                                55          55\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.063\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.968       0.958\n  Tucker-Lewis Index (TLI)                       0.947       0.930\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.965\n  Robust Tucker-Lewis Index (TLI)                            0.942\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6175.902   -6175.902\n  Scaling correction factor                                  1.128\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -6137.794   -6137.794\n  Scaling correction factor                                  1.023\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               12439.804   12439.804\n  Bayesian (BIC)                             12622.430   12622.430\n  Sample-size adjusted Bayesian (BIC)        12482.783   12482.783\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.053       0.059\n  90 Percent confidence interval - lower         0.037       0.043\n  90 Percent confidence interval - upper         0.068       0.075\n  P-value RMSEA <= 0.05                          0.360       0.173\n                                                                  \n  Robust RMSEA                                               0.055\n  90 Percent confidence interval - lower                     0.041\n  90 Percent confidence interval - upper                     0.070\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.046       0.046\n\n\n\nHierarchical model:\n\n\n\nCode\nfitted1_higher <- lavaan::cfa(\n  'Common =~ Updating + VerbalSTM + SpatialSTM\n  Updating =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `宇宙黑洞` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted1_higher, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0,\n  bifactor = \"Common\"\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted1_higher, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 34 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        36\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        45\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                119.456     116.993\n  Degrees of freedom                                 41          41\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   1.021\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1417.507    1333.075\n  Degrees of freedom                                55          55\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.063\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.942       0.941\n  Tucker-Lewis Index (TLI)                       0.923       0.920\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.943\n  Robust Tucker-Lewis Index (TLI)                            0.923\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6197.522   -6197.522\n  Scaling correction factor                                  1.025\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -6137.794   -6137.794\n  Scaling correction factor                                  1.023\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               12467.044   12467.044\n  Bayesian (BIC)                             12616.466   12616.466\n  Sample-size adjusted Bayesian (BIC)        12502.209   12502.209\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.064       0.063\n  90 Percent confidence interval - lower         0.051       0.050\n  90 Percent confidence interval - upper         0.077       0.076\n  P-value RMSEA <= 0.05                          0.042       0.053\n                                                                  \n  Robust RMSEA                                               0.064\n  90 Percent confidence interval - lower                     0.050\n  90 Percent confidence interval - upper                     0.077\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.037       0.037\n\n\n\n\nSpatial-object association test (宇宙黑洞) excluded\n\nCorrelated latent factor model:\n\n\n\nCode\nfitted2 <- lavaan::cfa(\n  'Updating =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted2, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted2, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 17 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        33\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        40\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                102.626     100.460\n  Degrees of freedom                                 32          32\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   1.022\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1322.205    1245.873\n  Degrees of freedom                                45          45\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.061\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.945       0.943\n  Tucker-Lewis Index (TLI)                       0.922       0.920\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.945\n  Robust Tucker-Lewis Index (TLI)                            0.923\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5608.669   -5608.669\n  Scaling correction factor                                  1.013\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5557.356   -5557.356\n  Scaling correction factor                                  1.017\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11283.338   11283.338\n  Bayesian (BIC)                             11420.308   11420.308\n  Sample-size adjusted Bayesian (BIC)        11315.573   11315.573\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.069       0.068\n  90 Percent confidence interval - lower         0.054       0.053\n  90 Percent confidence interval - upper         0.084       0.083\n  P-value RMSEA <= 0.05                          0.019       0.024\n                                                                  \n  Robust RMSEA                                               0.068\n  90 Percent confidence interval - lower                     0.053\n  90 Percent confidence interval - upper                     0.084\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.036       0.036\n\n\n\n\nCode\nfitted2_alt <- lavaan::cfa(\n  'Updating1 =~ `美术卡片` + `数字卡片`\n  Updating2 =~ `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted1_alt, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted1_alt, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 21 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        39\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        45\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                 67.870      66.281\n  Degrees of freedom                                 38          38\n  P-value (Chi-square)                            0.002       0.003\n  Scaling correction factor                                   1.024\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1417.507    1333.075\n  Degrees of freedom                                55          55\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.063\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.978       0.978\n  Tucker-Lewis Index (TLI)                       0.968       0.968\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.979\n  Robust Tucker-Lewis Index (TLI)                            0.969\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6171.729   -6171.729\n  Scaling correction factor                                  1.021\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -6137.794   -6137.794\n  Scaling correction factor                                  1.023\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               12421.459   12421.459\n  Bayesian (BIC)                             12583.332   12583.332\n  Sample-size adjusted Bayesian (BIC)        12459.554   12459.554\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.041       0.040\n  90 Percent confidence interval - lower         0.025       0.023\n  90 Percent confidence interval - upper         0.056       0.055\n  P-value RMSEA <= 0.05                          0.820       0.851\n                                                                  \n  Robust RMSEA                                               0.040\n  90 Percent confidence interval - lower                     0.023\n  90 Percent confidence interval - upper                     0.056\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.031       0.031\n\n\n\n\nCode\nfitted2_alt2 <- lavaan::cfa(\n  'Updating =~ Updating1 + Updating2\n  Updating1 =~ `美术卡片` + `数字卡片`\n  Updating2 =~ `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted2_alt2, what = \"std\", edge.color = \"black\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted2_alt2, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 35 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        35\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        40\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                 51.226      50.023\n  Degrees of freedom                                 30          30\n  P-value (Chi-square)                            0.009       0.012\n  Scaling correction factor                                   1.024\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1322.205    1245.873\n  Degrees of freedom                                45          45\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.061\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.983       0.983\n  Tucker-Lewis Index (TLI)                       0.975       0.975\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.984\n  Robust Tucker-Lewis Index (TLI)                            0.976\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5582.969   -5582.969\n  Scaling correction factor                                  1.011\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5557.356   -5557.356\n  Scaling correction factor                                  1.017\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11235.937   11235.937\n  Bayesian (BIC)                             11381.209   11381.209\n  Sample-size adjusted Bayesian (BIC)        11270.126   11270.126\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.039       0.038\n  90 Percent confidence interval - lower         0.019       0.018\n  90 Percent confidence interval - upper         0.057       0.055\n  P-value RMSEA <= 0.05                          0.838       0.865\n                                                                  \n  Robust RMSEA                                               0.038\n  90 Percent confidence interval - lower                     0.018\n  90 Percent confidence interval - upper                     0.056\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.029       0.029\n\n\n\nBifactor model:\n\n\n\nCode\nfitted2_bifac <- lavaan::cfa(\n  'Common =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片` + `幸运小球` + `密码箱` + `顺背数PRO` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`\n  Updating =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE, orthogonal = TRUE, \n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted2_bifac, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0,\n  bifactor = \"Common\"\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted2_bifac, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 46 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        40\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        40\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                 62.874      72.175\n  Degrees of freedom                                 25          25\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   0.871\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1322.205    1245.873\n  Degrees of freedom                                45          45\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.061\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.970       0.961\n  Tucker-Lewis Index (TLI)                       0.947       0.929\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.968\n  Robust Tucker-Lewis Index (TLI)                            0.942\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5588.793   -5588.793\n  Scaling correction factor                                  1.108\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5557.356   -5557.356\n  Scaling correction factor                                  1.017\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11257.586   11257.586\n  Bayesian (BIC)                             11423.610   11423.610\n  Sample-size adjusted Bayesian (BIC)        11296.659   11296.659\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.057       0.063\n  90 Percent confidence interval - lower         0.039       0.045\n  90 Percent confidence interval - upper         0.075       0.082\n  P-value RMSEA <= 0.05                          0.241       0.105\n                                                                  \n  Robust RMSEA                                               0.059\n  90 Percent confidence interval - lower                     0.044\n  90 Percent confidence interval - upper                     0.075\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.046       0.046\n\n\n\nHierarchical Model:\n\n\n\nCode\nfitted2_higher <- lavaan::cfa(\n  'Common =~ Updating + VerbalSTM + SpatialSTM\n  Updating =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted2_higher, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0,\n  bifactor = \"Common\"\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted2_higher, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 34 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        33\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        40\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                102.626     100.460\n  Degrees of freedom                                 32          32\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   1.022\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1322.205    1245.873\n  Degrees of freedom                                45          45\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.061\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.945       0.943\n  Tucker-Lewis Index (TLI)                       0.922       0.920\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.945\n  Robust Tucker-Lewis Index (TLI)                            0.923\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5608.669   -5608.669\n  Scaling correction factor                                  1.013\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5557.356   -5557.356\n  Scaling correction factor                                  1.017\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11283.338   11283.338\n  Bayesian (BIC)                             11420.308   11420.308\n  Sample-size adjusted Bayesian (BIC)        11315.573   11315.573\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.069       0.068\n  90 Percent confidence interval - lower         0.054       0.053\n  90 Percent confidence interval - upper         0.084       0.083\n  P-value RMSEA <= 0.05                          0.019       0.024\n                                                                  \n  Robust RMSEA                                               0.068\n  90 Percent confidence interval - lower                     0.053\n  90 Percent confidence interval - upper                     0.084\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.036       0.036\n\n\n\n\n\nTask Structure Model\n\nThe covariance estimate for latent variables is not correct. The estimated correlation between “Simple” and “Complex” is larger than 1.\n\n\n\nCode\nfitted3 <- lavaan::cfa(\n  'Complex =~ `打靶场` + `蝴蝶照相机` + `幸运小球`\n  Nback =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  Simple =~ `密码箱` + `顺背数PRO` + `位置记忆PRO`',\n  indices_selected, std.lv = TRUE, std.ov = TRUE, \n  estimator = \"MLR\", missing = \"ml\"\n)\n\n\nWarning in lav_object_post_check(object): lavaan WARNING: covariance matrix of latent variables\n                is not positive definite;\n                use lavInspect(fit, \"cov.lv\") to investigate.\n\n\nCode\nsemPlot::semPaths(\n  fitted3, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted3, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 18 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        33\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        40\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                175.560     172.586\n  Degrees of freedom                                 32          32\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   1.017\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1322.205    1245.873\n  Degrees of freedom                                45          45\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.061\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.888       0.883\n  Tucker-Lewis Index (TLI)                       0.842       0.835\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.888\n  Robust Tucker-Lewis Index (TLI)                            0.842\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -5645.136   -5645.136\n  Scaling correction factor                                  1.017\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -5557.356   -5557.356\n  Scaling correction factor                                  1.017\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               11356.271   11356.271\n  Bayesian (BIC)                             11493.241   11493.241\n  Sample-size adjusted Bayesian (BIC)        11388.506   11388.506\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.098       0.097\n  90 Percent confidence interval - lower         0.084       0.083\n  90 Percent confidence interval - upper         0.112       0.111\n  P-value RMSEA <= 0.05                          0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.098\n  90 Percent confidence interval - lower                     0.084\n  90 Percent confidence interval - upper                     0.112\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.054       0.054\n\n\n\n\nInput Material Model\n\nNot fitted well.\n\n\n\nCode\nfitted4 <- lavaan::cfa(\n  'Verbal =~ 数字卡片 + 文字卡片 + 幸运小球 + 密码箱 + 顺背数PRO\n  Spatial =~ 格子卡片 + 打靶场 + 蝴蝶照相机 + 位置记忆PRO\n  Object =~ 美术卡片 + 宇宙黑洞',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\n\n\nWarning in lav_object_post_check(object): lavaan WARNING: covariance matrix of latent variables\n                is not positive definite;\n                use lavInspect(fit, \"cov.lv\") to investigate.\n\n\nCode\nsemPlot::semPaths(\n  fitted4, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted4, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 20 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        36\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        45\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                318.493     323.033\n  Degrees of freedom                                 41          41\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   0.986\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1417.507    1333.075\n  Degrees of freedom                                55          55\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.063\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.796       0.779\n  Tucker-Lewis Index (TLI)                       0.727       0.704\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.795\n  Robust Tucker-Lewis Index (TLI)                            0.726\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6297.041   -6297.041\n  Scaling correction factor                                  1.065\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -6137.794   -6137.794\n  Scaling correction factor                                  1.023\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               12666.082   12666.082\n  Bayesian (BIC)                             12815.503   12815.503\n  Sample-size adjusted Bayesian (BIC)        12701.247   12701.247\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.120       0.121\n  90 Percent confidence interval - lower         0.108       0.109\n  90 Percent confidence interval - upper         0.133       0.134\n  P-value RMSEA <= 0.05                          0.000       0.000\n                                                                  \n  Robust RMSEA                                               0.120\n  90 Percent confidence interval - lower                     0.108\n  90 Percent confidence interval - upper                     0.133\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.070       0.070"
  },
  {
    "objectID": "posts/explore-struct-wm/index.html#correlation-matrix-3",
    "href": "posts/explore-struct-wm/index.html#correlation-matrix-3",
    "title": "Explore Cognitive Structure for Working Memory",
    "section": "Correlation Matrix",
    "text": "Correlation Matrix\n\n\nCode\ncorrelate(indices_selected, quiet = TRUE) |>\n  rearrange(method = \"HC\") |>\n  stretch() |>\n  mutate(across(c(x, y), as_factor)) |>\n  ggplot(aes(x, y)) +\n  geom_tile(aes(fill = r)) +\n  scico::scale_fill_scico(palette = \"bam\", midpoint = 0, direction = -1) +\n  coord_fixed() +\n  theme_minimal(base_size = 18) +\n  labs(x = \"\", y = \"\", fill = \"Pearson's\", color = \"\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\nCode\nmds <- indices_selected |>\n  cor(use = \"pairwise\") |>\n  smacof::sim2diss(to.dist = TRUE) |>\n  smacof::mds(ndim = 2, type = \"mspline\")\n# plot(mds, plot.type = \"Shepard\", main = \"Shepard Diagram (Ratio Transformation)\")\n# par(family = \"SimHei\")\nplot(mds)"
  },
  {
    "objectID": "posts/explore-struct-wm/index.html#exploratory-factor-analysis-3",
    "href": "posts/explore-struct-wm/index.html#exploratory-factor-analysis-3",
    "title": "Explore Cognitive Structure for Working Memory",
    "section": "Exploratory Factor Analysis",
    "text": "Exploratory Factor Analysis\n\nTraditional\n\n\nCode\nnfactors_test <- psych::nfactors(indices_selected)\n\n\n\n\n\n\n\nCode\nfit <- psych::fa(indices_selected, 4)\npsych::fa.diagram(fit)\n\n\n\n\n\n\n\nCode\nfit_bifac <- psych::omega(indices_selected, 4, plot = FALSE)\npsych::omega.diagram(fit_bifac)\n\n\n\n\n\n\n\nBayesian Factor Analysis\n\n\nCode\nmcmc <- indices_selected |>\n  mutate(across(.fns = ~ scale(.)[, 1])) |>\n  befa(verbose = FALSE) |>\n  post.column.switch() |>\n  post.sign.switch()\n\n\n\n\nCode\nhppm <- summary(mcmc, what = \"hppm\")\nhppm |>\n  pluck(\"alpha\", \"m1\") |>\n  as_tibble(rownames = \"alpha_term\") |>\n  separate(alpha_term, c(NA, \"game_index\"), sep = \":\") |>\n  mutate(game_index = reorder(game_index, dedic)) |>\n  ggplot(aes(game_index, dedic)) +\n  geom_tile(aes(fill = mean)) +\n  geom_text(aes(label = round(mean, 2)), color = \"white\") +\n  scico::scale_fill_scico(palette = \"bam\", midpoint = 0, direction = -1) +\n  coord_fixed() +\n  theme_minimal(base_size = 18) +\n  labs(x = \"Term\", y = \"Factor\",\n       title = str_c(\"Posterior Probability: \", round(hppm$hppm$prob, 2),\n                     \", with \", hppm$hppm$nfac, \" factors\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "posts/explore-struct-wm/index.html#confirmatory-factor-analysis-3",
    "href": "posts/explore-struct-wm/index.html#confirmatory-factor-analysis-3",
    "title": "Explore Cognitive Structure for Working Memory",
    "section": "Confirmatory Factor Analysis",
    "text": "Confirmatory Factor Analysis\n\n\nCode\nfit_result <- lavaan::cfa(\n  'Updating =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `宇宙黑洞` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`\n  Reasoning =~ 三维心理旋转测试 + 图形推理 + 图形折叠',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fit_result, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fit_result, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 21 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        48\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        72\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                172.314     172.556\n  Degrees of freedom                                 71          71\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   0.999\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1615.500    1546.634\n  Degrees of freedom                                91          91\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.045\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.934       0.930\n  Tucker-Lewis Index (TLI)                       0.915       0.911\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.933\n  Robust Tucker-Lewis Index (TLI)                            0.915\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7811.990   -7811.990\n  Scaling correction factor                                  1.051\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -7725.833   -7725.833\n  Scaling correction factor                                  1.020\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               15719.980   15719.980\n  Bayesian (BIC)                             15919.209   15919.209\n  Sample-size adjusted Bayesian (BIC)        15766.867   15766.867\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.055       0.055\n  90 Percent confidence interval - lower         0.045       0.045\n  90 Percent confidence interval - upper         0.066       0.066\n  P-value RMSEA <= 0.05                          0.200       0.197\n                                                                  \n  Robust RMSEA                                               0.055\n  90 Percent confidence interval - lower                     0.045\n  90 Percent confidence interval - upper                     0.066\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.040       0.040\n\n\n\n\nCode\nfit_result_alt <- lavaan::cfa(\n  'Updating =~ Updating1 + Updating2\n  Updating1 =~ `美术卡片` + `数字卡片`\n  Updating2 =~ `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `宇宙黑洞` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`\n  Reasoning =~ 三维心理旋转测试 + 图形推理 + 图形折叠',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fit_result_alt, what = \"std\", edge.color = \"black\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fit_result_alt, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 35 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        50\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        72\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                121.402     121.672\n  Degrees of freedom                                 69          69\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   0.998\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1615.500    1546.634\n  Degrees of freedom                                91          91\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.045\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.966       0.964\n  Tucker-Lewis Index (TLI)                       0.955       0.952\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.965\n  Robust Tucker-Lewis Index (TLI)                            0.954\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7786.534   -7786.534\n  Scaling correction factor                                  1.050\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -7725.833   -7725.833\n  Scaling correction factor                                  1.020\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               15673.068   15673.068\n  Bayesian (BIC)                             15880.598   15880.598\n  Sample-size adjusted Bayesian (BIC)        15721.908   15721.908\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.040       0.040\n  90 Percent confidence interval - lower         0.028       0.028\n  90 Percent confidence interval - upper         0.052       0.052\n  P-value RMSEA <= 0.05                          0.914       0.911\n                                                                  \n  Robust RMSEA                                               0.040\n  90 Percent confidence interval - lower                     0.028\n  90 Percent confidence interval - upper                     0.052\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.036       0.036\n\n\n\n\nCode\nfit_result_high_order <- lavaan::cfa(\n  'WM =~ Updating + VerbalSTM + SpatialSTM\n  Updating =~ Updating1 + Updating2\n  Updating1 =~ `美术卡片` + `数字卡片`\n  Updating2 =~ `格子卡片` + `文字卡片`\n  # AscMem =~ `人工语言-中级` + `欢乐餐厅` + 图片记忆\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `打靶场` + `蝴蝶照相机` + `位置记忆PRO`\n  Reasoning =~ 三维心理旋转测试 + 图形推理 + 图形折叠',\n  indices_selected, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fit_result_high_order, what = \"std\", edge.color = \"black\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0\n)\n\n\n\n\n\nCode\nlavaan::summary(fit_result_high_order, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 49 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        45\n                                                      \n  Number of observations                           469\n  Number of missing patterns                        68\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                109.216     109.773\n  Degrees of freedom                                 59          59\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   0.995\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1511.617    1452.531\n  Degrees of freedom                                78          78\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.041\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.965       0.963\n  Tucker-Lewis Index (TLI)                       0.954       0.951\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.965\n  Robust Tucker-Lewis Index (TLI)                            0.953\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -7204.293   -7204.293\n  Scaling correction factor                                  1.041\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -7149.685   -7149.685\n  Scaling correction factor                                  1.015\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               14498.586   14498.586\n  Bayesian (BIC)                             14685.363   14685.363\n  Sample-size adjusted Bayesian (BIC)        14542.543   14542.543\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.043       0.043\n  90 Percent confidence interval - lower         0.030       0.030\n  90 Percent confidence interval - upper         0.055       0.055\n  P-value RMSEA <= 0.05                          0.830       0.821\n                                                                  \n  Robust RMSEA                                               0.043\n  90 Percent confidence interval - lower                     0.030\n  90 Percent confidence interval - upper                     0.055\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.039       0.039"
  },
  {
    "objectID": "posts/Large-scale meta analysis/index.html",
    "href": "posts/Large-scale meta analysis/index.html",
    "title": "Extracting Cognitive Factors from Large-Scale Meta-Anslysis Data",
    "section": "",
    "text": "To summarize briefly, I followed the step of Beam et al. (2021, Nature Neuroscience) to find evidence for the structure of human cognition. Firstly, filtering all studies that included the testing paradigms that consisted with those used in our project from the database of Neurosynth, I conducted 28 meta-analysis separately on each group of fMRI studies. Thanks to NiMARE (a python research environment for neuroimaging meta-analysis), I could easily perform Multilevel Kernel Density Analysis (MKDA) on hundreds fMRI studies without manually coding.\nHere is the summarize table for the number of included fMRI studies in each meta-analysis."
  },
  {
    "objectID": "posts/Large-scale meta analysis/index.html#traditional",
    "href": "posts/Large-scale meta analysis/index.html#traditional",
    "title": "Extracting Cognitive Factors from Large-Scale Meta-Anslysis Data",
    "section": "Traditional",
    "text": "Traditional\n\n\nCode\nnfactors_test <- psych::nfactors(cor_nonzero)\n\n\n\n\n\n\nCorrelated latent factors\n\n\n\nCode\nfit <- psych::fa(cor_nonzero, 4)\npsych::fa.diagram(fit)\n\n\n\n\n\n\nBifactor model\n\n\n\nCode\nfit_bifac <- psych::omega(total_nonzero, 3, plot = FALSE)\npsych::omega.diagram(fit_bifac)"
  },
  {
    "objectID": "posts/prepare-training/index.html",
    "href": "posts/prepare-training/index.html",
    "title": "Prepare Tasks for Cognitive Training Project",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(corrr)\nlibrary(formattable)\nrequireNamespace(\"bit64\")\nA basic principle choosing tasks for measuring training transfer effect is based on the correlations between the chosen tasks and the target tasks. Two types of target tasks are now selected:\nHere the correlations with these two types of tasks are calculated as follows."
  },
  {
    "objectID": "posts/prepare-training/index.html#rapm",
    "href": "posts/prepare-training/index.html#rapm",
    "title": "Prepare Tasks for Cognitive Training Project",
    "section": "RAPM",
    "text": "RAPM\n\n\nCode\nindices_clean |> \n  filter(game_name_abbr != \"RAPM\") |> \n  inner_join(\n    indices_clean |> \n      filter(\n        game_name_abbr == \"RAPM\", \n        index_name == \"nc_test\"\n      ) |> \n      select(user_id, rapm = test),\n    by = \"user_id\"\n  ) |> \n  group_by(game_name, index_name) |> \n  summarise(\n    n = sum(!is.na(test) & !is.na(rapm)),\n    r = cor(test, rapm, use = \"complete\"),\n    .groups = \"drop\"\n  ) |> \n  mutate(abs_r = abs(r)) |> \n  arrange(desc(abs_r)) |> \n  left_join(\n    reliability_test_retest |> \n      select(game_name, index_name, icc = icc_no_outlier),\n    by = c(\"game_name\", \"index_name\")\n  ) |> \n  format_dt()"
  },
  {
    "objectID": "posts/prepare-training/index.html#nonverbal-reasoning",
    "href": "posts/prepare-training/index.html#nonverbal-reasoning",
    "title": "Prepare Tasks for Cognitive Training Project",
    "section": "Nonverbal Reasoning",
    "text": "Nonverbal Reasoning\n\n\nCode\nindices_clean |> \n  filter(game_name_abbr != \"NVR\") |> \n  inner_join(\n    indices_clean |> \n      filter(game_name_abbr == \"NVR\") |> \n      select(user_id, nvr = test),\n    by = \"user_id\"\n  ) |> \n  group_by(game_name, index_name) |> \n  summarise(\n    n = sum(!is.na(test) & !is.na(nvr)),\n    r = cor(test, nvr, use = \"complete\"),\n    .groups = \"drop\"\n  ) |> \n  filter(n > 100) |> \n  mutate(abs_r = abs(r)) |> \n  arrange(desc(abs_r)) |> \n  left_join(\n    reliability_test_retest |> \n      select(game_name, index_name, icc = icc_no_outlier),\n    by = c(\"game_name\", \"index_name\")\n  ) |> \n  format_dt()"
  },
  {
    "objectID": "posts/prepare-training/index.html#the-latent-factor-model-heirachical-model",
    "href": "posts/prepare-training/index.html#the-latent-factor-model-heirachical-model",
    "title": "Prepare Tasks for Cognitive Training Project",
    "section": "The Latent Factor Model (heirachical model)",
    "text": "The Latent Factor Model (heirachical model)\n\n\nCode\nfitted <- lavaan::cfa(\n  'Common =~ Updating + VerbalSTM + SpatialSTM\n  Updating =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `宇宙黑洞` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_viswm, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0,\n  bifactor = \"Common\"\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 34 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        36\n                                                      \n  Number of observations                           515\n  Number of missing patterns                        26\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                119.832     114.108\n  Degrees of freedom                                 41          41\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   1.050\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1493.661    1199.093\n  Degrees of freedom                                55          55\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.246\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.945       0.936\n  Tucker-Lewis Index (TLI)                       0.926       0.914\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.946\n  Robust Tucker-Lewis Index (TLI)                            0.928\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6490.246   -6490.246\n  Scaling correction factor                                  1.867\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -6430.330   -6430.330\n  Scaling correction factor                                  1.432\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               13052.493   13052.493\n  Bayesian (BIC)                             13205.283   13205.283\n  Sample-size adjusted Bayesian (BIC)        13091.013   13091.013\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.061       0.059\n  90 Percent confidence interval - lower         0.049       0.046\n  90 Percent confidence interval - upper         0.074       0.071\n  P-value RMSEA <= 0.05                          0.071       0.116\n                                                                  \n  Robust RMSEA                                               0.060\n  90 Percent confidence interval - lower                     0.047\n  90 Percent confidence interval - upper                     0.074\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.038       0.038"
  },
  {
    "objectID": "posts/prepare-training/index.html#common-factor",
    "href": "posts/prepare-training/index.html#common-factor",
    "title": "Prepare Tasks for Cognitive Training Project",
    "section": "Common Factor",
    "text": "Common Factor\n\n\nCode\nscores_latent <- bind_cols(\n  select(indices_viswm, user_id),\n  lavaan::predict(fitted) |> \n    unclass() |> \n    as_tibble()\n)\nindices_clean |> \n  inner_join(\n    scores_latent,\n    by = \"user_id\"\n  ) |> \n  group_by(game_name, index_name) |> \n  summarise(\n    n = sum(!is.na(test) & !is.na(Common)),\n    r = cor(test, Common, use = \"complete\"),\n    .groups = \"drop\"\n  ) |> \n  mutate(abs_r = abs(r)) |> \n  arrange(desc(abs_r)) |> \n  left_join(\n    reliability_test_retest |> \n      select(game_name, index_name, icc = icc_no_outlier),\n    by = c(\"game_name\", \"index_name\")\n  ) |> \n  format_dt()"
  },
  {
    "objectID": "posts/prepare-training/index.html#the-latent-factor-model-hierarchical-model",
    "href": "posts/prepare-training/index.html#the-latent-factor-model-hierarchical-model",
    "title": "Prepare Tasks for Cognitive Training Project",
    "section": "The Latent Factor Model (hierarchical model)",
    "text": "The Latent Factor Model (hierarchical model)\n\n\nCode\nfitted <- lavaan::cfa(\n  'Common =~ Updating + VerbalSTM + SpatialSTM\n  Updating =~ `美术卡片` + `数字卡片` + `格子卡片` + `文字卡片`\n  VerbalSTM =~ `幸运小球` + `密码箱` + `顺背数PRO`\n  SpatialSTM =~ `宇宙黑洞` + `打靶场` + `蝴蝶照相机` + `位置记忆PRO`',\n  indices_viswm, std.lv = TRUE, std.ov = TRUE,\n  estimator = \"MLR\", missing = \"ml\"\n)\nsemPlot::semPaths(\n  fitted, what = \"std\", edge.color = \"black\", layout = \"tree2\",\n  sizeMan = 6, sizeLat = 8, edge.label.cex = 0.6, intercepts = FALSE,\n  nCharEdges = 5, esize = 1, trans = 1, nCharNodes = 0,\n  bifactor = \"Common\"\n)\n\n\n\n\n\nCode\nlavaan::summary(fitted, fit.measures = TRUE, estimates = FALSE)\n\n\nlavaan 0.6-11 ended normally after 34 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        36\n                                                      \n  Number of observations                           515\n  Number of missing patterns                        26\n                                                      \nModel Test User Model:\n                                               Standard      Robust\n  Test Statistic                                119.832     114.108\n  Degrees of freedom                                 41          41\n  P-value (Chi-square)                            0.000       0.000\n  Scaling correction factor                                   1.050\n       Yuan-Bentler correction (Mplus variant)                     \n\nModel Test Baseline Model:\n\n  Test statistic                              1493.661    1199.093\n  Degrees of freedom                                55          55\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.246\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.945       0.936\n  Tucker-Lewis Index (TLI)                       0.926       0.914\n                                                                  \n  Robust Comparative Fit Index (CFI)                         0.946\n  Robust Tucker-Lewis Index (TLI)                            0.928\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6490.246   -6490.246\n  Scaling correction factor                                  1.867\n      for the MLR correction                                      \n  Loglikelihood unrestricted model (H1)      -6430.330   -6430.330\n  Scaling correction factor                                  1.432\n      for the MLR correction                                      \n                                                                  \n  Akaike (AIC)                               13052.493   13052.493\n  Bayesian (BIC)                             13205.283   13205.283\n  Sample-size adjusted Bayesian (BIC)        13091.013   13091.013\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.061       0.059\n  90 Percent confidence interval - lower         0.049       0.046\n  90 Percent confidence interval - upper         0.074       0.071\n  P-value RMSEA <= 0.05                          0.071       0.116\n                                                                  \n  Robust RMSEA                                               0.060\n  90 Percent confidence interval - lower                     0.047\n  90 Percent confidence interval - upper                     0.074\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.038       0.038"
  },
  {
    "objectID": "posts/prepare-training/index.html#average-of-these-two-tasks",
    "href": "posts/prepare-training/index.html#average-of-these-two-tasks",
    "title": "Prepare Tasks for Cognitive Training Project",
    "section": "Average of these two tasks",
    "text": "Average of these two tasks\n\n\nCode\nmean_scores <- indices_clean |> \n  filter(\n    game_name_abbr == \"NVR\" |\n      (game_name_abbr == \"RAPM\" & index_name == \"nc_test\")\n  ) |> \n  pivot_wider(\n    id_cols = user_id,\n    names_from = game_name_abbr,\n    values_from = test\n  ) |> \n  mutate(\n    across(c(NVR, RAPM), ~ scale(.)[, 1]),\n    score_avg = map2_dbl(NVR, RAPM, ~ (.x + .y) / 2)\n  )\nindices_clean |> \n  filter(!game_name_abbr %in% c(\"NVR\", \"RAPM\")) |> \n  inner_join(mean_scores, by = \"user_id\") |> \n  group_by(game_name, index_name) |> \n  summarise(\n    n = sum(!is.na(test) & !is.na(score_avg)),\n    r = cor(test, score_avg, use = \"complete\"),\n    .groups = \"drop\"\n  ) |> \n  filter(n > 100) |> \n  mutate(abs_r = abs(r)) |> \n  arrange(desc(abs_r)) |> \n  left_join(\n    reliability_test_retest |> \n      select(game_name, index_name, icc = icc_no_outlier),\n    by = c(\"game_name\", \"index_name\")\n  ) |> \n  format_dt()"
  },
  {
    "objectID": "posts/prepare-training/index.html#strength-for-each-task",
    "href": "posts/prepare-training/index.html#strength-for-each-task",
    "title": "Prepare Tasks for Cognitive Training Project",
    "section": "Strength for each task",
    "text": "Strength for each task\n\n\nCode\nlibrary(tidygraph)\ngraph <- indices_clean |> \n  pivot_wider_indices() |> \n  select(-user_id) |> \n  correlate() |> \n  stretch(na.rm = TRUE, remove.dups = TRUE) |> \n  filter(r > 0.15) |> \n  tidygraph::as_tbl_graph(directed = FALSE)\nstrengths <- graph |> \n  activate(nodes) |> \n  mutate(strength = centrality_degree(weights = r)) |> \n  as_tibble() |> \n  arrange(desc(strength))\nstrengths |> \n  mutate(strength = digits(strength, 1)) |> \n  formattable(\n    list(strength = color_text(\"green\", \"red\"))\n  ) |> \n  as.datatable()\n\n\n\n\n\n\n\nCode\nstrengths |> \n  ggplot(aes(fct_reorder(name, strength, .desc = TRUE), strength)) +\n  geom_bar(stat = \"identity\") +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(x = \"Task Name\", y = \"Strength (Weighted Degree)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "posts/prepare-training/index.html#strength-for-each-task-node",
    "href": "posts/prepare-training/index.html#strength-for-each-task-node",
    "title": "Prepare Tasks for Cognitive Training Project",
    "section": "Strength for each task node",
    "text": "Strength for each task node\n\n\nCode\nlibrary(tidygraph)\ngraph <- indices_clean |> \n  pivot_wider_indices() |> \n  select(-user_id) |> \n  correlate() |> \n  stretch(na.rm = TRUE, remove.dups = TRUE) |> \n  filter(r > 0.15) |> \n  tidygraph::as_tbl_graph(directed = FALSE)\nstrengths <- graph |> \n  activate(nodes) |> \n  mutate(strength = centrality_degree(weights = r)) |> \n  as_tibble() |> \n  arrange(desc(strength))\nstrengths |> \n  mutate(strength = digits(strength, 1)) |> \n  formattable(\n    list(strength = color_text(\"green\", \"red\"))\n  ) |> \n  as.datatable()\n\n\n\n\n\n\n\nCode\nstrengths |> \n  ggplot(aes(fct_reorder(name, strength, .desc = TRUE), strength)) +\n  geom_bar(stat = \"identity\") +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(x = \"Task Name\", y = \"Strength (Weighted Degree)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "posts/prepare-training/index.html#clustering-tasks",
    "href": "posts/prepare-training/index.html#clustering-tasks",
    "title": "Prepare Tasks for Cognitive Training Project",
    "section": "Clustering tasks",
    "text": "Clustering tasks\nTry to cluster tasks into three clusters, and select tasks from the clusters.\n\n\nCode\nsuppressPackageStartupMessages(library(dendextend))\nclust <- cog_data |> \n  cor(use = \"pairwise\") |> \n  smacof::sim2diss(to.dist = TRUE) |> \n  hclust()\n# dend <- as.dendrogram(clust)\ndend_k <- find_k(clust)\nplot(dend_k)\n\n\n\n\n\nCode\ndend <- clust |> \n  color_branches(k = dend_k$nc) |> \n  set(\"labels_cex\", 0.4)\nplot(dend)\nrect.dendrogram(dend, k = dend_k$nc, border = 8)"
  },
  {
    "objectID": "posts/prepare-training/index.html#factor-analysis",
    "href": "posts/prepare-training/index.html#factor-analysis",
    "title": "Prepare Tasks for Cognitive Training Project",
    "section": "Factor Analysis",
    "text": "Factor Analysis\n使用探索性因素分析的思路对于任务选取进行量化的定义。对于控制组、训练组的训练任务，以及近迁移、远迁移测试任务，我们的思路是：（1）训练组任务应与近迁移任务、远迁移任务拥有共同成分（Domain-general），而控制组训练任务与迁移任务没有共同成分；（2）训练组任务应与近迁移任务在Bi-factor模型中拥有共同的任务特殊性成分（Domain-specific）；（3）训练组任务应与远迁移任务在Bi-factor模型中处于不同的任务特殊性成分中。\n\nStep 1: finding general factor\n在第一步中，我们对认知测试包含的所有任务具有代表性的指标进行探索性因素分析，并将factor数设置为1，意味着我们筛选出具有至少1个共同成分的任务作为训练组和迁移任务的备选。无法找出共同成分的任务，我们设置为控制组训练任务（factor的载荷低于0.4）。\n\n\nCode\nfit <- psych::fa(cog_data)\ng_loadings <- fit$loadings |> \n  unclass() |> \n  as_tibble(rownames = \"game_index\")\n\n\n基于此规则，我们得到不与其他任务具有共同general成分的任务如下。这些任务可以用作控制组训练任务。因为这些任务与其他任务并不能找到共同的general factor，如果符合预期，那么控制组的训练并不会对其他任务产生影响。\n\n\nCode\ng_loadings |> \n  filter(abs(MR1) < 0.4) |> \n  mutate(MR1 = digits(MR1, 2)) |> \n  formattable() |> \n  as.datatable(\n    options = list(pageLength = 15)\n  )\n\n\n\n\n\n\n\n\n\nStep 2: Bifactor model for domain-specific\n下面，将筛选出的具有共同成分的任务使用Bi-factor模型，寻找domain-specific的因子。\n\n\nCode\ng_indices <- g_loadings |> \n  filter(abs(MR1) > 0.4) |> \n  pull(game_index)\ncog_data_g <- cog_data |> \n  select(all_of(g_indices))\nfactor_test <- psych::nfactors(cog_data_g)\n\n\n\n\n\n检验5个specific因子的模型：\n\nCorrelated factor model\n\n\n\nCode\nfit_fa <- psych::fa(cog_data_g, 4)\npsych::fa.diagram(fit_fa)\n\n\n\n\n\n\nBi-factor model\n\n\n\nCode\nfit_bifac <- psych::omega(cog_data_g, 4, plot = FALSE)\npsych::omega.diagram(fit_bifac)\n\n\n\n\n\n按照我们的思路，训练组的训练任务可从bi-factor模型中的specific因子包含的任务中选取，近迁移任务可以选择为同一specific因子内部的任务，而远迁移任务可以选定为属于不同specific因子的任务。这样选定的原理是：（1）近迁移任务与训练任务既有domain-general factor的共享，也有domain-specific因子的共享；（2）而远迁移任务与训练任务只存在domain-general因子的重叠，而使用不同的domain-specific因子。其中部分任务会出现共载荷现象，将不选择这些任务。"
  },
  {
    "objectID": "posts/prepare-training/index.html#step-2-bifactor-model-for-domain-specific",
    "href": "posts/prepare-training/index.html#step-2-bifactor-model-for-domain-specific",
    "title": "Prepare Tasks for Cognitive Training Project",
    "section": "Step 2: Bifactor model for domain-specific",
    "text": "Step 2: Bifactor model for domain-specific\n下面，将筛选出的具有共同成分的任务使用Bi-factor模型，寻找domain-specific的因子。\n\n\nCode\ng_indices <- g_loadings |> \n  filter(abs(MR1) > 0.3) |> \n  pull(game_index)\ncog_data_g <- cog_data |> \n  select(all_of(g_indices))\nfactor_test <- psych::nfactors(cog_data_g)\n\n\n\n\n\n经检验，这些任务包含4到6个domain-specific因子。首先检验4个specific因子的模型：\n\n\nCode\nfit_bifac <- psych::omega(cog_data_g, 4, plot = FALSE)\npsych::omega.diagram(fit_bifac)\n\n\n\n\n\n其次检验5个specific因子的模型：\n\n\nCode\nfit_bifac <- psych::omega(cog_data_g, 5, plot = FALSE)\npsych::omega.diagram(fit_bifac)\n\n\n\n\n\n最后为6个因子的模型：\n\n\nCode\nfit_bifac <- psych::omega(cog_data_g, 6, plot = FALSE)\npsych::omega.diagram(fit_bifac)\n\n\n\n\n\n按照我们的思路，训练组的训练任务可从bi-factor模型中的specific因子包含的任务中选取，近迁移任务可以选择为同一specific因子内部的任务，而远迁移任务可以选定为属于不同specific因子的任务。这样选定的原理是：（1）近迁移任务与训练任务既有domain-general factor的共享，也有domain-specific因子的共享；（2）而远迁移任务与训练任务只存在domain-general因子的重叠，而使用不同的domain-specific因子。"
  }
]